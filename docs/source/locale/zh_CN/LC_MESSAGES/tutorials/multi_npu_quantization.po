# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-kunlun team
# This file is distributed under the same license as the vllm-kunlun
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version:  vllm-kunlun\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-10 16:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/tutorials/multi_npu_quantization.md:1
msgid "Multi-XPU (QwQ 32B W8A8)"
msgstr "多XPU（QwQ 32B W8A8）"

#: ../../source/tutorials/multi_npu_quantization.md:3
#, fuzzy
msgid "Run Docker Container"
msgstr "运行 docker 容器"

#~ msgid "w8a8 quantization feature is supported by v0.8.4rc2 or higher"
#~ msgstr "w8a8 量化功能由 v0.8.4rc2 或更高版本支持"

#~ msgid "Install modelslim and convert model"
#~ msgstr "安装 modelslim 并转换模型"

#~ msgid ""
#~ "You can choose to convert the "
#~ "model yourself or use the quantized "
#~ "model we uploaded,  see "
#~ "https://www.modelscope.cn/models/vllm-kunlun/QwQ-32B-"
#~ "W8A8"
#~ msgstr ""
#~ "你可以选择自己转换模型，或者使用我们上传的量化模型，详见 https://www.modelscope.cn/models"
#~ "/vllm-kunlun/QwQ-32B-W8A8"

#~ msgid "Verify the quantized model"
#~ msgstr "验证量化模型"

#~ msgid "The converted model files looks like:"
#~ msgstr "转换后的模型文件如下所示："

#~ msgid "Run the following script to start the vLLM server with quantized model:"
#~ msgstr "运行以下脚本以启动带有量化模型的 vLLM 服务器："

#~ msgid ""
#~ "The value \"kunlun\" for \"--"
#~ "quantization\" argument will be supported "
#~ "after [a specific PR](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/877) is merged and"
#~ " released, you can cherry-pick this"
#~ " commit for now."
#~ msgstr ""
#~ "在 [特定的PR](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/877) 合并并发布后， \"--quantization\" "
#~ "参数将支持值 \"kunlun\"，你也可以现在手动挑选该提交。"

#~ msgid "Once your server is started, you can query the model with input prompts"
#~ msgstr "一旦服务器启动，就可以通过输入提示词来查询模型。"

#~ msgid ""
#~ "Run the following script to execute "
#~ "offline inference on multi-XPU with "
#~ "quantized model:"
#~ msgstr "运行以下脚本，在多XPU上使用量化模型执行离线推理："

#~ msgid ""
#~ "To enable quantization for kunlun, "
#~ "quantization method must be \"kunlun\""
#~ msgstr "要在kunlun上启用量化，量化方法必须为“kunlun”。"

