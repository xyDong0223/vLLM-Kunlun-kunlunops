# Chinese translations for PROJECT.
# Copyright (C) 2025 ORGANIZATION
# This file is distributed under the same license as the PROJECT project.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PROJECT VERSION\n"
"Report-Msgid-Bugs-To: EMAIL@ADDRESS\n"
"POT-Creation-Date: 2025-11-10 16:59+0800\n"
"PO-Revision-Date: 2025-07-18 10:11+0800\n"
"Last-Translator: \n"
"Language: zh\n"
"Language-Team: \n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/user_guide/release_notes.md:1
#, fuzzy
msgid "Release Notes"
msgstr "ç‰ˆæœ¬è¯´æ˜"

#~ msgid "v0.9.2rc1 - 2025.07.11"
#~ msgstr ""

#~ msgid ""
#~ "This is the 1st release candidate "
#~ "of v0.9.2 for vLLM Kunlun. Please "
#~ "follow the [official doc](https://vllm-"
#~ "kunlun.readthedocs.io/en/) to get started. "
#~ "From this release, V1 engine will "
#~ "be enabled by default, there is no"
#~ " need to set `VLLM_USE_V1=1` any "
#~ "more. And this release is the last"
#~ " version to support V0 engine, V0 "
#~ "code will be clean up in the "
#~ "future."
#~ msgstr ""
#~ "è¿™æ˜¯ vLLM Kunlun v0.9.2 "
#~ "çš„ç¬¬ä¸€ä¸ªå€™é€‰å‘å¸ƒç‰ˆæœ¬ã€‚è¯·å‚é˜…[å®˜æ–¹æ–‡æ¡£](https://vllm-"
#~ "kunlun.readthedocs.io/en/)å¼€å§‹ä½¿ç”¨ã€‚ä»æœ¬æ¬¡å‘å¸ƒèµ·ï¼ŒV1 å¼•æ“å°†é»˜è®¤å¯ç”¨ï¼Œä¸å†éœ€è¦è®¾ç½® "
#~ "`VLLM_USE_V1=1`ã€‚æ­¤å¤–ï¼Œè¯¥ç‰ˆæœ¬ä¹Ÿæ˜¯æœ€åä¸€ä¸ªæ”¯æŒ V0 å¼•æ“çš„ç‰ˆæœ¬ï¼ŒV0 "
#~ "ç›¸å…³ä»£ç å°†åœ¨æœªæ¥è¢«æ¸…ç†ã€‚"

#~ msgid "Highlights"
#~ msgstr "äº®ç‚¹"

#~ msgid ""
#~ "Pooling model works with V1 engine "
#~ "now. You can take a try with "
#~ "Qwen3 embedding model [#1359](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1359)."
#~ msgstr ""
#~ "Pooling æ¨¡å‹ç°åœ¨å¯ä»¥ä¸ V1 å¼•æ“ä¸€èµ·ä½¿ç”¨ã€‚ä½ å¯ä»¥å°è¯•ä½¿ç”¨ Qwen3 "
#~ "embedding æ¨¡å‹ [#1359](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1359)ã€‚"

#~ msgid ""
#~ "The performance on Atlas 300I series "
#~ "has been improved. [#1591](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1591)"
#~ msgstr ""
#~ "Atlas 300I ç³»åˆ—çš„æ€§èƒ½å·²ç»æå‡ã€‚ [#1591](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1591)"

#~ msgid ""
#~ "aclgraph mode works with Moe models "
#~ "now. Currently, only Qwen3 Moe is "
#~ "well tested. [#1381](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1381)"
#~ msgstr ""
#~ "aclgraph æ¨¡å¼ç°åœ¨å¯ä»¥ä¸ Moe æ¨¡å‹ä¸€èµ·ä½¿ç”¨ã€‚ç›®å‰ï¼Œä»…å¯¹ Qwen3 "
#~ "Moe è¿›è¡Œäº†å……åˆ†æµ‹è¯•ã€‚[#1381](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1381)"

#~ msgid "Core"
#~ msgstr "æ ¸å¿ƒ"

#~ msgid ""
#~ "Kunlun PyTorch adapter (torch_npu) has "
#~ "been upgraded to `2.5.1.post1.dev20250619`. "
#~ "Donâ€™t forget to update it in your"
#~ " environment. [#1347](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1347)"
#~ msgstr ""
#~ "Kunlun PyTorch é€‚é…å™¨ï¼ˆtorch_npuï¼‰å·²å‡çº§åˆ° "
#~ "`2.5.1.post1.dev20250619`ã€‚è¯·ä¸è¦å¿˜è®°åœ¨æ‚¨çš„ç¯å¢ƒä¸­è¿›è¡Œæ›´æ–°ã€‚ "
#~ "[#1347](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1347)"

#~ msgid ""
#~ "The **GatherV3** error has been fixed"
#~ " with **aclgraph** mode. "
#~ "[#1416](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1416)"
#~ msgstr ""
#~ "**GatherV3** é”™è¯¯å·²é€šè¿‡ **aclgraph** "
#~ "æ¨¡å¼ä¿®å¤ã€‚[#1416](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1416)"

#~ msgid ""
#~ "W8A8 quantization works on Atlas 300I"
#~ " series now. [#1560](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/1560)"
#~ msgstr ""
#~ "W8A8 é‡åŒ–ç°åœ¨å¯ä»¥åœ¨ Atlas 300I "
#~ "ç³»åˆ—ä¸Šè¿è¡Œäº†ã€‚[#1560](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1560)"

#~ msgid ""
#~ "Fix the accuracy problem with deploy "
#~ "models with parallel parameters. "
#~ "[#1678](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1678)"
#~ msgstr ""
#~ "ä¿®å¤äº†ä½¿ç”¨å¹¶è¡Œå‚æ•°éƒ¨ç½²æ¨¡å‹æ—¶çš„å‡†ç¡®æ€§é—®é¢˜ã€‚[#1678](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1678)"

#~ msgid ""
#~ "The pre-built wheel package now "
#~ "requires lower version of glibc. Users"
#~ " can use it by `pip install "
#~ "vllm-kunlun` directly. [#1582](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1582)"
#~ msgstr ""
#~ "é¢„ç¼–è¯‘çš„ wheel åŒ…ç°åœ¨è¦æ±‚æ›´ä½ç‰ˆæœ¬çš„ glibcã€‚ç”¨æˆ·å¯ä»¥ç›´æ¥é€šè¿‡ `pip "
#~ "install vllm-kunlun` ä½¿ç”¨å®ƒã€‚[#1582](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1582)"

#~ msgid "Other"
#~ msgstr "å…¶å®ƒ"

#~ msgid ""
#~ "Official doc has been updated for "
#~ "better read experience. For example, "
#~ "more deployment tutorials are added, "
#~ "user/developer docs are updated. More "
#~ "guide will coming soon."
#~ msgstr "å®˜æ–¹æ–‡æ¡£å·²æ›´æ–°ï¼Œä»¥æå‡é˜…è¯»ä½“éªŒã€‚ä¾‹å¦‚ï¼Œå¢åŠ äº†æ›´å¤šéƒ¨ç½²æ•™ç¨‹ï¼Œç”¨æˆ·/å¼€å‘è€…æ–‡æ¡£å·²æ›´æ–°ã€‚æ›´å¤šæŒ‡å—å³å°†æ¨å‡ºã€‚"

#~ msgid ""
#~ "Fix accuracy problem for deepseek V3/R1"
#~ " models with torchair graph in long"
#~ " sequence predictions. [#1331](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1331)"
#~ msgstr ""
#~ "ä¿®å¤ deepseek V3/R1 æ¨¡å‹åœ¨ä½¿ç”¨ torchair "
#~ "å›¾è¿›è¡Œé•¿åºåˆ—é¢„æµ‹æ—¶çš„ç²¾åº¦é—®é¢˜ã€‚[#1331](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1331)"

#~ msgid ""
#~ "A new env variable "
#~ "`VLLM_ENABLE_FUSED_EXPERTS_ALLGATHER_EP` has been "
#~ "added. It enables the fused "
#~ "allgather-experts kernel for Deepseek V3/R1"
#~ " models. The default value is `0`."
#~ " [#1335](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1335)"
#~ msgstr ""
#~ "æ–°å¢äº†ä¸€ä¸ªç¯å¢ƒå˜é‡ `VLLM_ENABLE_FUSED_EXPERTS_ALLGATHER_EP`ã€‚å®ƒç”¨äºå¯ç”¨ "
#~ "Deepseek V3/R1 æ¨¡å‹çš„ fused allgather-"
#~ "experts å†…æ ¸ã€‚é»˜è®¤å€¼ä¸º `0`ã€‚[#1335](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/1335)"

#~ msgid ""
#~ "A new env variable "
#~ "`VLLM_KUNLUN_ENABLE_TOPK_TOPP_OPTIMIZATION` has been "
#~ "added to improve the performance of "
#~ "topk-topp sampling. The default value"
#~ " is 0, we'll consider to enable "
#~ "it by default in the "
#~ "future[#1732](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1732)"
#~ msgstr ""
#~ "æ–°å¢äº†ä¸€ä¸ªç¯å¢ƒå˜é‡ `VLLM_KUNLUN_ENABLE_TOPK_TOPP_OPTIMIZATION`ï¼Œç”¨äºæå‡ "
#~ "topk-topp é‡‡æ ·çš„æ€§èƒ½ã€‚è¯¥å˜é‡é»˜è®¤å€¼ä¸º "
#~ "0ï¼Œæœªæ¥æˆ‘ä»¬ä¼šè€ƒè™‘é»˜è®¤å¯ç”¨æ­¤é€‰é¡¹[#1732](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1732)ã€‚"

#~ msgid ""
#~ "A batch of bugs have been fixed"
#~ " for Data Parallelism case "
#~ "[#1273](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1273) [#1322](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1322) [#1275](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/1275) [#1478](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1478)"
#~ msgstr ""
#~ "å·²ä¿®å¤äº†ä¸€æ‰¹ä¸æ•°æ®å¹¶è¡Œç›¸å…³çš„ bug [#1273](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/1273) [#1322](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1322) "
#~ "[#1275](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1275) [#1478](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1478)"

#~ msgid ""
#~ "The DeepSeek performance has been "
#~ "improved. [#1194](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1194) [#1395](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1395) [#1380](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/1380)"
#~ msgstr ""
#~ "DeepSeek çš„æ€§èƒ½å·²å¾—åˆ°æå‡ã€‚[#1194](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1194) [#1395](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/1395) [#1380](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1380)"

#~ msgid ""
#~ "Kunlun scheduler works with prefix cache"
#~ " now. [#1446](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1446)"
#~ msgstr ""
#~ "Kunlun è°ƒåº¦å™¨ç°åœ¨æ”¯æŒå‰ç¼€ç¼“å­˜ã€‚[#1446](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1446)"

#~ msgid ""
#~ "DeepSeek now works with prefix cache "
#~ "now. [#1498](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1498)"
#~ msgstr ""
#~ "DeepSeek ç°åœ¨æ”¯æŒå‰ç¼€ç¼“å­˜äº†ã€‚[#1498](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1498)"

#~ msgid ""
#~ "Support prompt logprobs to recover ceval"
#~ " accuracy in V1 [#1483](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1483)"
#~ msgstr ""
#~ "æ”¯æŒä½¿ç”¨ prompt logprobs æ¢å¤ V1 çš„ ceval"
#~ " å‡†ç¡®ç‡ [#1483](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1483)"

#~ msgid "v0.9.1rc1 - 2025.06.22"
#~ msgstr "v0.9.1rc1 - 2025.06.22"

#~ msgid ""
#~ "This is the 1st release candidate "
#~ "of v0.9.1 for vLLM Kunlun. Please "
#~ "follow the [official doc](https://vllm-"
#~ "kunlun.readthedocs.io/en/) to get started."
#~ msgstr ""
#~ "è¿™æ˜¯ vLLM Kunlun v0.9.1 "
#~ "çš„ç¬¬ä¸€ä¸ªå€™é€‰å‘å¸ƒç‰ˆæœ¬ã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-"
#~ "kunlun.readthedocs.io/en/)å¼€å§‹ä½¿ç”¨ã€‚"

#~ msgid ""
#~ "Atlas 300I series is experimental "
#~ "supported in this release. "
#~ "[#1333](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1333) After careful consideration, "
#~ "this feature **will NOT be included "
#~ "in v0.9.1-dev branch** taking into "
#~ "account the v0.9.1 release quality and"
#~ " the feature rapid iteration to "
#~ "improve performance on Atlas 300I "
#~ "series. We will improve this from "
#~ "0.9.2rc1 and later."
#~ msgstr ""
#~ "æœ¬ç‰ˆæœ¬å¯¹ Atlas 300I "
#~ "ç³»åˆ—æä¾›äº†å®éªŒæ€§æ”¯æŒã€‚[#1333](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1333) ç»è¿‡æ…é‡è€ƒè™‘ï¼Œé‰´äº v0.9.1 ç‰ˆæœ¬å‘å¸ƒçš„è´¨é‡è¦æ±‚ä»¥åŠ "
#~ "Atlas 300I ç³»åˆ—æ€§èƒ½ä¼˜åŒ–çš„å¿«é€Ÿè¿­ä»£ï¼Œè¯¥åŠŸèƒ½**ä¸ä¼šè¢«åŒ…å«åœ¨ v0.9.1-dev "
#~ "åˆ†æ”¯ä¸­**ã€‚æˆ‘ä»¬å°†åœ¨ 0.9.2rc1 åŠä¹‹åçš„ç‰ˆæœ¬ä¸­è¿›ä¸€æ­¥å®Œå–„è¯¥åŠŸèƒ½ã€‚"

#~ msgid ""
#~ "Support EAGLE-3 for speculative decoding. "
#~ "[#1032](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1032)"
#~ msgstr ""
#~ "æ”¯æŒ EAGLE-3 è¿›è¡Œæ¨æµ‹å¼è§£ç ã€‚[#1032](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/1032)"

#~ msgid ""
#~ "Kunlun PyTorch adapter (torch_npu) has "
#~ "been upgraded to `2.5.1.post1.dev20250528`. "
#~ "Donâ€™t forget to update it in your"
#~ " environment. [#1235](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1235)"
#~ msgstr ""
#~ "Kunlun PyTorch é€‚é…å™¨ï¼ˆtorch_npuï¼‰å·²å‡çº§åˆ° "
#~ "`2.5.1.post1.dev20250528`ã€‚è¯·ä¸è¦å¿˜è®°åœ¨æ‚¨çš„ç¯å¢ƒä¸­è¿›è¡Œæ›´æ–°ã€‚[#1235](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1235)"

#~ msgid ""
#~ "Support Atlas 300I series container "
#~ "image. You can get it from "
#~ "[quay.io](https://quay.io/repository/vllm/vllm-kunlun)"
#~ msgstr ""
#~ "æ”¯æŒAtlas "
#~ "300Iç³»åˆ—çš„å®¹å™¨é•œåƒã€‚ä½ å¯ä»¥ä»[quay.io](https://quay.io/repository/vllm/vllm-"
#~ "kunlun)è·å–ã€‚"

#~ msgid ""
#~ "Fix token-wise padding mechanism to "
#~ "make multi-card graph mode work. "
#~ "[#1300](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1300)"
#~ msgstr ""
#~ "ä¿®å¤æŒ‰ token å¡«å……æœºåˆ¶ä»¥æ”¯æŒå¤šå¡å›¾æ¨¡å¼ã€‚ [#1300](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1300)"

#~ msgid ""
#~ "Upgrade vllm to 0.9.1 "
#~ "[#1165]https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1165"
#~ msgstr ""
#~ "å°† vllm å‡çº§åˆ° 0.9.1 [#1165]https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1165"

#~ msgid "Other Improvements"
#~ msgstr "å…¶ä»–æ”¹è¿›"

#~ msgid ""
#~ "Initial support Chunked Prefill for MLA."
#~ " [#1172](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1172)"
#~ msgstr ""
#~ "ä¸ºMLAåˆæ­¥æ”¯æŒåˆ†å—é¢„å¡«å……ã€‚ [#1172](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1172)"

#~ msgid ""
#~ "An example of best practices to "
#~ "run DeepSeek with ETP has been "
#~ "added. [#1101](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1101)"
#~ msgstr ""
#~ "å·²æ–°å¢ä¸€ä¸ªä½¿ç”¨ ETP è¿è¡Œ DeepSeek "
#~ "çš„æœ€ä½³å®è·µç¤ºä¾‹ã€‚[#1101](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1101)"

#~ msgid ""
#~ "Performance improvements for DeepSeek using"
#~ " the TorchAir graph. [#1098](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1098), "
#~ "[#1131](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1131)"
#~ msgstr ""
#~ "é€šè¿‡ä½¿ç”¨ TorchAir å›¾å¯¹ DeepSeek "
#~ "è¿›è¡Œäº†æ€§èƒ½æå‡ã€‚[#1098](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1098), [#1131](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1131)"

#~ msgid ""
#~ "Supports the speculative decoding feature "
#~ "with KunlunScheduler. [#943](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/943)"
#~ msgstr ""
#~ "æ”¯æŒ KunlunScheduler çš„é¢„æµ‹æ€§è§£ç åŠŸèƒ½ã€‚[#943](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/943)"

#~ msgid ""
#~ "Improve `VocabParallelEmbedding` custom op "
#~ "performance. It will be enabled in "
#~ "the next release. [#796](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/796)"
#~ msgstr ""
#~ "æå‡ `VocabParallelEmbedding` "
#~ "è‡ªå®šä¹‰ç®—å­çš„æ€§èƒ½ã€‚è¯¥ä¼˜åŒ–å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­å¯ç”¨ã€‚[#796](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/796)"

#~ msgid ""
#~ "Fixed a device discovery and setup "
#~ "bug when running vLLM Kunlun on "
#~ "Ray [#884](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/884)"
#~ msgstr ""
#~ "ä¿®å¤äº†åœ¨ Ray ä¸Šè¿è¡Œ vLLM Kunlun æ—¶çš„è®¾å¤‡å‘ç°å’Œè®¾ç½®é”™è¯¯ "
#~ "[#884](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/884)"

#~ msgid ""
#~ "DeepSeek with "
#~ "[MC2](https://www.hikunlun.com/document/detail/zh/canncommercial/81RC1/developmentguide/opdevg/kunluncbestP/atlas_kunlunc_best_practices_10_0043.html)"
#~ " (Merged Compute and Communication) now "
#~ "works properly. [#1268](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/1268)"
#~ msgstr ""
#~ "DeepSeek ç°å·²å¯ä»¥ä¸ "
#~ "[MC2](https://www.hikunlun.com/document/detail/zh/canncommercial/81RC1/developmentguide/opdevg/kunluncbestP/atlas_kunlunc_best_practices_10_0043.html)ï¼ˆè®¡ç®—ä¸é€šä¿¡èåˆï¼‰æ­£å¸¸å·¥ä½œã€‚[#1268](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1268)"

#~ msgid ""
#~ "Fixed log2phy NoneType bug with static"
#~ " EPLB feature. [#1186](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/1186)"
#~ msgstr ""
#~ "ä¿®å¤äº†å¸¦æœ‰é™æ€ EPLB ç‰¹æ€§æ—¶ log2phy ä¸º NoneType "
#~ "çš„ bugã€‚[#1186](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1186)"

#~ msgid ""
#~ "Improved performance for DeepSeek with "
#~ "DBO enabled. [#997](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/997), [#1135](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/1135)"
#~ msgstr ""
#~ "å¯ç”¨ DBO åï¼ŒDeepSeek çš„æ€§èƒ½å¾—åˆ°æå‡ã€‚[#997](https://github.com"
#~ "/vllm-project/vllm-"
#~ "kunlun/pull/997)ï¼Œ[#1135](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1135)"

#~ msgid ""
#~ "Refactoring KunlunFusedMoE [#1229](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1229)"
#~ msgstr ""
#~ "é‡æ„ KunlunFusedMoE [#1229](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/1229)"

#~ msgid ""
#~ "Add initial user stories page (include"
#~ " LLaMA-Factory/TRL/verl/MindIE Turbo/GPUStack) "
#~ "[#1224](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1224)"
#~ msgstr ""
#~ "æ–°å¢åˆå§‹ç”¨æˆ·æ•…äº‹é¡µé¢ï¼ˆåŒ…æ‹¬ LLaMA-Factory/TRL/verl/MindIE "
#~ "Turbo/GPUStackï¼‰[#1224](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1224)"

#~ msgid ""
#~ "Add unit test framework "
#~ "[#1201](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1201)"
#~ msgstr "æ·»åŠ å•å…ƒæµ‹è¯•æ¡†æ¶ [#1201](https://github.com/vllm-project/vllm-kunlun/pull/1201)"

#~ msgid "Known Issues"
#~ msgstr "å·²çŸ¥é—®é¢˜"

#~ msgid ""
#~ "In some cases, the vLLM process "
#~ "may crash with a **GatherV3** error "
#~ "when **aclgraph** is enabled. We are "
#~ "working on this issue and will fix"
#~ " it in the next release. "
#~ "[#1038](https://github.com/vllm-project/vllm-"
#~ "kunlun/issues/1038)"
#~ msgstr ""
#~ "åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå½“å¯ç”¨ **aclgraph** æ—¶ï¼ŒvLLM è¿›ç¨‹å¯èƒ½ä¼šå›  "
#~ "**GatherV3** "
#~ "é”™è¯¯è€Œå´©æºƒã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³æ­¤é—®é¢˜ï¼Œå¹¶å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­ä¿®å¤ã€‚[#1038](https://github.com/vllm-"
#~ "project/vllm-kunlun/issues/1038)"

#~ msgid ""
#~ "Prefix cache feature does not work "
#~ "with the Kunlun Scheduler but without"
#~ " chunked prefill enabled. This will "
#~ "be fixed in the next release. "
#~ "[#1350](https://github.com/vllm-project/vllm-"
#~ "kunlun/issues/1350)"
#~ msgstr ""
#~ "å‰ç¼€ç¼“å­˜åŠŸèƒ½åœ¨æœªå¯ç”¨åˆ†å—é¢„å¡«å……çš„æƒ…å†µä¸‹æ— æ³•ä¸ Kunlun "
#~ "è°ƒåº¦å™¨ä¸€åŒå·¥ä½œã€‚æ­¤é—®é¢˜å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­ä¿®å¤ã€‚[#1350](https://github.com/vllm-project"
#~ "/vllm-kunlun/issues/1350)"

#~ msgid "Full Changelog"
#~ msgstr "å®Œæ•´æ›´æ–°æ—¥å¿—"

#~ msgid ""
#~ "https://github.com/vllm-project/vllm-"
#~ "kunlun/compare/v0.9.0rc2...v0.9.1rc1"
#~ msgstr ""
#~ "https://github.com/vllm-project/vllm-"
#~ "kunlun/compare/v0.9.0rc2...v0.9.1rc1"

#~ msgid "v0.9.0rc2 - 2025.06.10"
#~ msgstr "v0.9.0rc2 - 2025.06.10"

#~ msgid ""
#~ "This release contains some quick fixes"
#~ " for v0.9.0rc1. Please use this "
#~ "release instead of v0.9.0rc1."
#~ msgstr "æœ¬æ¬¡å‘å¸ƒåŒ…å«äº†ä¸€äº›é’ˆå¯¹ v0.9.0rc1 çš„å¿«é€Ÿä¿®å¤ã€‚è¯·ä½¿ç”¨æœ¬æ¬¡å‘å¸ƒç‰ˆæœ¬ï¼Œè€Œä¸æ˜¯ v0.9.0rc1ã€‚"

#~ msgid ""
#~ "Fix the import error when vllm-"
#~ "kunlun is installed without editable "
#~ "way. [#1152](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1152)"
#~ msgstr ""
#~ "ä¿®å¤å½“ä»¥éå¯ç¼–è¾‘æ–¹å¼å®‰è£… vllm-kunlun "
#~ "æ—¶çš„å¯¼å…¥é”™è¯¯ã€‚[#1152](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1152)"

#~ msgid "v0.9.0rc1 - 2025.06.09"
#~ msgstr "v0.9.0rc1 - 2025.06.09"

#~ msgid ""
#~ "This is the 1st release candidate "
#~ "of v0.9.0 for vllm-kunlun. Please "
#~ "follow the [official doc](https://vllm-"
#~ "kunlun.readthedocs.io/en/) to start the "
#~ "journey. From this release, V1 Engine"
#~ " is recommended to use. The code "
#~ "of V0 Engine is frozen and will"
#~ " not be maintained any more. Please"
#~ " set environment `VLLM_USE_V1=1` to enable"
#~ " V1 Engine."
#~ msgstr ""
#~ "è¿™æ˜¯ vllm-kunlun v0.9.0 "
#~ "çš„ç¬¬ä¸€ä¸ªå€™é€‰å‘å¸ƒç‰ˆæœ¬ã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-"
#~ "kunlun.readthedocs.io/en/)å¼€å§‹ä½¿ç”¨ã€‚ä»æ­¤ç‰ˆæœ¬èµ·ï¼Œæ¨èä½¿ç”¨ V1 å¼•æ“ã€‚V0 "
#~ "å¼•æ“çš„ä»£ç å·²è¢«å†»ç»“ï¼Œä¸å†ç»´æŠ¤ã€‚å¦‚éœ€å¯ç”¨ V1 å¼•æ“ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ `VLLM_USE_V1=1`ã€‚"

#~ msgid ""
#~ "DeepSeek works with graph mode now. "
#~ "Follow the [official doc](https://vllm-"
#~ "kunlun.readthedocs.io/en/latest/user_guide/feature_guide/graph_mode.html)"
#~ " to take a try. [#789](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/789)"
#~ msgstr ""
#~ "DeepSeek ç°åœ¨å·²æ”¯æŒå›¾æ¨¡å¼ã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-"
#~ "kunlun.readthedocs.io/en/latest/user_guide/feature_guide/graph_mode.html)è¿›è¡Œå°è¯•ã€‚[#789](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/789)"

#~ msgid ""
#~ "Qwen series models works with graph "
#~ "mode now. It works by default with"
#~ " V1 Engine. Please note that in "
#~ "this release, only Qwen series models"
#~ " are well tested with graph mode. "
#~ "We'll make it stable and generalize "
#~ "in the next release. If you hit"
#~ " any issues, please feel free to "
#~ "open an issue on GitHub and "
#~ "fallback to eager mode temporarily by"
#~ " set `enforce_eager=True` when initializing "
#~ "the model."
#~ msgstr ""
#~ "Qwen ç³»åˆ—æ¨¡å‹ç°åœ¨æ”¯æŒå›¾æ¨¡å¼ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒåœ¨ V1 å¼•æ“ä¸‹è¿è¡Œã€‚è¯·æ³¨æ„ï¼Œæœ¬æ¬¡å‘å¸ƒä¸­ï¼Œä»… "
#~ "Qwen "
#~ "ç³»åˆ—æ¨¡å‹ç»è¿‡äº†å……åˆ†çš„å›¾æ¨¡å¼æµ‹è¯•ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­è¿›ä¸€æ­¥æå‡å…¶ç¨³å®šæ€§å¹¶æ¨å¹¿è‡³æ›´å¹¿æ³›çš„åœºæ™¯ã€‚å¦‚æœä½ é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶åœ¨ "
#~ "GitHub ä¸Šæäº¤ issueï¼Œå¹¶åœ¨åˆå§‹åŒ–æ¨¡å‹æ—¶é€šè¿‡è®¾ç½® `enforce_eager=True`"
#~ " ä¸´æ—¶åˆ‡æ¢å› eager æ¨¡å¼ã€‚"

#~ msgid ""
#~ "The performance of multi-step scheduler"
#~ " has been improved. Thanks for the"
#~ " contribution from China Merchants Bank."
#~ " [#814](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/814)"
#~ msgstr ""
#~ "å¤šæ­¥è°ƒåº¦å™¨çš„æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚æ„Ÿè°¢æ‹›å•†é“¶è¡Œçš„è´¡çŒ®ã€‚[#814](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/814)"

#~ msgid ""
#~ "LoRAã€Multi-LoRA And Dynamic Serving is"
#~ " supported for V1 Engine now. Thanks"
#~ " for the contribution from China "
#~ "Merchants Bank. [#893](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/893)"
#~ msgstr ""
#~ "V1 å¼•æ“ç°åœ¨æ”¯æŒ LoRAã€å¤š LoRA "
#~ "ä»¥åŠåŠ¨æ€æœåŠ¡ã€‚æ„Ÿè°¢æ‹›å•†é“¶è¡Œçš„è´¡çŒ®ã€‚[#893](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/893)"

#~ msgid ""
#~ "Prefix cache and chunked prefill feature"
#~ " works now [#782](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/782) [#844](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/844)"
#~ msgstr ""
#~ "å‰ç¼€ç¼“å­˜å’Œåˆ†å—é¢„å¡«å……åŠŸèƒ½ç°å·²å¯ç”¨ [#782](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/782) [#844](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/844)"

#~ msgid ""
#~ "Spec decode and MTP features work "
#~ "with V1 Engine now. [#874](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/874) "
#~ "[#890](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/890)"
#~ msgstr ""
#~ "Spec è§£ç å’Œ MTP åŠŸèƒ½ç°åœ¨å·²ç»æ”¯æŒ V1 "
#~ "å¼•æ“ã€‚[#874](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/874) [#890](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/890)"

#~ msgid ""
#~ "DP feature works with DeepSeek now. "
#~ "[#1012](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/1012)"
#~ msgstr ""
#~ "DP åŠŸèƒ½ç°åœ¨å¯ä»¥ä¸ DeepSeek ä¸€èµ·ä½¿ç”¨ã€‚[#1012](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/1012)"

#~ msgid ""
#~ "Input embedding feature works with V0"
#~ " Engine now. [#916](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/916)"
#~ msgstr ""
#~ "è¾“å…¥åµŒå…¥ç‰¹æ€§ç°åœ¨å·²æ”¯æŒ V0 å¼•æ“ã€‚[#916](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/916)"

#~ msgid ""
#~ "Sleep mode feature works with V1 "
#~ "Engine now. [#1084](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1084)"
#~ msgstr ""
#~ "ä¼‘çœ æ¨¡å¼åŠŸèƒ½ç°åœ¨å·²æ”¯æŒ V1 å¼•æ“ã€‚[#1084](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/1084)"

#~ msgid "Model"
#~ msgstr "æ¨¡å‹"

#~ msgid ""
#~ "Qwen2.5 VL works with V1 Engine "
#~ "now. [#736](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/736)"
#~ msgstr ""
#~ "Qwen2.5 VL ç°åœ¨å¯ä»¥ä¸ V1 "
#~ "å¼•æ“ååŒå·¥ä½œã€‚[#736](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/736)"

#~ msgid ""
#~ "LLama4 works now. [#740](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/740)"
#~ msgstr ""
#~ "LLama4 ç°åœ¨å¯ä»¥ä½¿ç”¨äº†ã€‚[#740](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/740)"

#~ msgid ""
#~ "A new kind of DeepSeek model "
#~ "called dual-batch overlap(DBO) is added."
#~ " Please set `VLLM_KUNLUN_ENABLE_DBO=1` to "
#~ "use it. [#941](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/941)"
#~ msgstr ""
#~ "æ–°å¢äº†ä¸€ç§åä¸ºåŒæ‰¹æ¬¡é‡å ï¼ˆdual-batch overlapï¼ŒDBOï¼‰çš„ DeepSeek "
#~ "æ¨¡å‹ã€‚è¯·è®¾ç½® `VLLM_KUNLUN_ENABLE_DBO=1` ä»¥å¯ç”¨ã€‚ "
#~ "[#941](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/941)"

#~ msgid ""
#~ "online serve with kunlun quantization "
#~ "works now. [#877](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/877)"
#~ msgstr ""
#~ "åœ¨çº¿æœåŠ¡ç°å·²æ”¯æŒKunluné‡åŒ–ã€‚[#877](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/877)"

#~ msgid ""
#~ "A batch of bugs for graph mode "
#~ "and moe model have been fixed. "
#~ "[#773](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/773) [#771](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/771) [#774](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/774) [#816](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/816) "
#~ "[#817](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/817) [#819](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/819) [#912](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/912) [#897](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/897) "
#~ "[#961](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/961) [#958](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/958) [#913](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/913) [#905](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/905)"
#~ msgstr ""
#~ "å·²ä¿®å¤ä¸€æ‰¹å…³äºå›¾æ¨¡å¼å’Œmoeæ¨¡å‹çš„bugã€‚[#773](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/773) [#771](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/771) [#774](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/774) "
#~ "[#816](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/816) [#817](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/817) [#819](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/819) [#912](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/912) "
#~ "[#897](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/897) [#961](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/961) [#958](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/958) [#913](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/913) "
#~ "[#905](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/905)"

#~ msgid ""
#~ "A batch of performance improvement PRs"
#~ " have been merged. [#784](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/784) "
#~ "[#803](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/803) [#966](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/966) [#839](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/839) [#970](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/970) "
#~ "[#947](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/947) [#987](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/987) [#1085](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/1085)"
#~ msgstr ""
#~ "ä¸€æ‰¹æ€§èƒ½æ”¹è¿›çš„ PR å·²è¢«åˆå¹¶ã€‚[#784](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/784) [#803](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/803) "
#~ "[#966](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/966) [#839](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/839) [#970](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/970) [#947](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/947) "
#~ "[#987](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/987) [#1085](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/1085)"

#~ msgid ""
#~ "From this release, binary wheel package"
#~ " will be released as well. "
#~ "[#775](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/775)"
#~ msgstr ""
#~ "ä»æœ¬ç‰ˆæœ¬å¼€å§‹ï¼Œå°†åŒæ—¶å‘å¸ƒäºŒè¿›åˆ¶ wheel åŒ…ã€‚[#775](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/775)"

#~ msgid ""
#~ "The contributor doc site is "
#~ "[added](https://vllm-"
#~ "kunlun.readthedocs.io/en/latest/community/contributors.html)"
#~ msgstr ""
#~ "è´¡çŒ®è€…æ–‡æ¡£ç«™ç‚¹å·²[æ·»åŠ ](https://vllm-"
#~ "kunlun.readthedocs.io/en/latest/community/contributors.html)"

#~ msgid "Known Issue"
#~ msgstr "å·²çŸ¥é—®é¢˜"

#~ msgid ""
#~ "In some case, vLLM process may be"
#~ " crashed with aclgraph enabled. We're "
#~ "working this issue and it'll be "
#~ "fixed in the next release."
#~ msgstr "åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¯ç”¨ aclgraph æ—¶ vLLM è¿›ç¨‹å¯èƒ½ä¼šå´©æºƒã€‚æˆ‘ä»¬æ­£åœ¨å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œå¹¶å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­ä¿®å¤ã€‚"

#~ msgid ""
#~ "Multi node data-parallel doesn't work"
#~ " with this release. This is a "
#~ "known issue in vllm and has been"
#~ " fixed on main branch. "
#~ "[#18981](https://github.com/vllm-project/vllm/pull/18981)"
#~ msgstr ""
#~ "å¤šèŠ‚ç‚¹æ•°æ®å¹¶è¡Œåœ¨æ­¤ç‰ˆæœ¬ä¸­æ— æ³•ä½¿ç”¨ã€‚è¿™æ˜¯ vllm ä¸­å·²çŸ¥çš„é—®é¢˜ï¼Œå¹¶å·²åœ¨ä¸»åˆ†æ”¯ä¸­ä¿®å¤ã€‚ "
#~ "[#18981](https://github.com/vllm-project/vllm/pull/18981)"

#~ msgid "v0.7.3.post1 - 2025.05.29"
#~ msgstr "v0.7.3.post1 - 2025.05.29"

#~ msgid ""
#~ "This is the first post release of"
#~ " 0.7.3. Please follow the [official "
#~ "doc](https://vllm-kunlun.readthedocs.io/en/v0.7.3-dev) to"
#~ " start the journey. It includes the"
#~ " following changes:"
#~ msgstr ""
#~ "è¿™æ˜¯ 0.7.3 çš„ç¬¬ä¸€ä¸ªè¡¥ä¸å‘å¸ƒã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev)å¼€å§‹ä½¿ç”¨ã€‚æœ¬æ¬¡æ›´æ–°åŒ…æ‹¬ä»¥ä¸‹æ›´æ”¹ï¼š"

#~ msgid ""
#~ "Qwen3 and Qwen3MOE is supported now. "
#~ "The performance and accuracy of Qwen3"
#~ " is well tested. You can try it"
#~ " now. Mindie Turbo is recomanded to"
#~ " improve the performance of Qwen3. "
#~ "[#903](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/903) [#915](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/915)"
#~ msgstr ""
#~ "ç°åœ¨å·²æ”¯æŒ Qwen3 å’Œ Qwen3MOEã€‚Qwen3 "
#~ "çš„æ€§èƒ½å’Œç²¾åº¦å·²ç»è¿‡å……åˆ†æµ‹è¯•ï¼Œä½ å¯ä»¥ç«‹å³è¯•ç”¨ã€‚æ¨èä½¿ç”¨ Mindie Turbo ä»¥æå‡ "
#~ "Qwen3 çš„æ€§èƒ½ã€‚[#903](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/903) [#915](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/915)"

#~ msgid ""
#~ "Added a new performance guide. The "
#~ "guide aims to help users to "
#~ "improve vllm-kunlun performance on "
#~ "system level. It includes OS "
#~ "configuration, library optimization, deploy "
#~ "guide and so on. [#878](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/878) [Doc "
#~ "Link](https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev/developer_guide/performance/optimization_and_tuning.html)"
#~ msgstr ""
#~ "æ–°å¢äº†ä¸€ä¸ªæ€§èƒ½æŒ‡å—ã€‚è¯¥æŒ‡å—æ—¨åœ¨å¸®åŠ©ç”¨æˆ·åœ¨ç³»ç»Ÿå±‚é¢æå‡ vllm-kunlun "
#~ "çš„æ€§èƒ½ã€‚å†…å®¹åŒ…æ‹¬æ“ä½œç³»ç»Ÿé…ç½®ã€åº“ä¼˜åŒ–ã€éƒ¨ç½²æŒ‡å—ç­‰ã€‚ [#878](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/878) [æ–‡æ¡£é“¾æ¥](https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev/developer_guide/performance/optimization_and_tuning.html)"

#~ msgid "Bug Fix"
#~ msgstr "æ¼æ´ä¿®å¤"

#~ msgid ""
#~ "Qwen2.5-VL  works for RLHF scenarios "
#~ "now. [#928](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/928)"
#~ msgstr ""
#~ "Qwen2.5-VL ç°åœ¨å·²æ”¯æŒ RLHF åœºæ™¯ã€‚[#928](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/928)"

#~ msgid ""
#~ "Users can launch the model from "
#~ "online weights now. e.g. from "
#~ "huggingface or modelscope directly "
#~ "[#858](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/858) [#918](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/918)"
#~ msgstr ""
#~ "ç”¨æˆ·ç°åœ¨å¯ä»¥ç›´æ¥ä»åœ¨çº¿æƒé‡å¯åŠ¨æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç›´æ¥ä» huggingface æˆ– modelscope"
#~ " è·å–ã€‚[#858](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/858) [#918](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/918)"

#~ msgid ""
#~ "The meaningless log info `UserWorkspaceSize0`"
#~ " has been cleaned. [#911](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/911)"
#~ msgstr ""
#~ "æ— æ„ä¹‰çš„æ—¥å¿—ä¿¡æ¯ `UserWorkspaceSize0` "
#~ "å·²è¢«æ¸…ç†ã€‚[#911](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/911)"

#~ msgid ""
#~ "The log level for `Failed to "
#~ "import vllm_kunlun_C` has been changed "
#~ "to `warning` instead of `error`. "
#~ "[#956](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/956)"
#~ msgstr ""
#~ "`Failed to import vllm_kunlun_C` çš„æ—¥å¿—çº§åˆ«å·²ä» "
#~ "`error` æ›´æ”¹ä¸º `warning`ã€‚[#956](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/956)"

#~ msgid ""
#~ "DeepSeek MLA now works with chunked "
#~ "prefill in V1 Engine. Please note "
#~ "that V1 engine in 0.7.3 is just"
#~ " expermential and only for test "
#~ "usage. [#849](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/849) [#936](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/936)"
#~ msgstr ""
#~ "DeepSeek MLA ç°å·²åœ¨ V1 å¼•æ“ä¸­æ”¯æŒåˆ†å—é¢„å¡«å……ã€‚è¯·æ³¨æ„ï¼Œ0.7.3 "
#~ "ç‰ˆæœ¬ä¸­çš„ V1 å¼•æ“ä»…ä¸ºå®éªŒæ€§ï¼Œä»…ä¾›æµ‹è¯•ä½¿ç”¨ã€‚[#849](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/849) [#936](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/936)"

#~ msgid "Docs"
#~ msgstr "æ–‡æ¡£"

#~ msgid ""
#~ "The benchmark doc is updated for "
#~ "Qwen2.5 and Qwen2.5-VL [#792](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/792)"
#~ msgstr ""
#~ "åŸºå‡†æ–‡æ¡£å·²é’ˆå¯¹ Qwen2.5 å’Œ Qwen2.5-VL æ›´æ–° "
#~ "[#792](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/792)"

#~ msgid ""
#~ "Add the note to clear that only"
#~ " \"modelscope<1.23.0\" works with 0.7.3. "
#~ "[#954](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/954)"
#~ msgstr ""
#~ "æ·»åŠ è¯´æ˜ï¼Œæ˜ç¡®åªæœ‰ \"modelscope<1.23.0\" èƒ½ä¸ 0.7.3 "
#~ "ä¸€èµ·ä½¿ç”¨ã€‚[#954](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/954)"

#~ msgid "v0.7.3 - 2025.05.08"
#~ msgstr "v0.7.3 - 2025.05.08"

#~ msgid "ğŸ‰ Hello, World!"
#~ msgstr "ğŸ‰ ä½ å¥½ï¼Œä¸–ç•Œï¼"

#~ msgid ""
#~ "We are excited to announce the "
#~ "release of 0.7.3 for vllm-kunlun. "
#~ "This is the first official release. "
#~ "The functionality, performance, and stability"
#~ " of this release are fully tested "
#~ "and verified. We encourage you to "
#~ "try it out and provide feedback. "
#~ "We'll post bug fix versions in the"
#~ " future if needed. Please follow the"
#~ " [official doc](https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev) to start the"
#~ " journey."
#~ msgstr ""
#~ "æˆ‘ä»¬å¾ˆé«˜å…´åœ°å®£å¸ƒ vllm-kunlun 0.7.3 "
#~ "ç‰ˆæœ¬æ­£å¼å‘å¸ƒã€‚è¿™æ˜¯é¦–ä¸ªæ­£å¼å‘å¸ƒçš„ç‰ˆæœ¬ã€‚è¯¥ç‰ˆæœ¬çš„åŠŸèƒ½ã€æ€§èƒ½å’Œç¨³å®šæ€§å·²å……åˆ†æµ‹è¯•å’ŒéªŒè¯ã€‚æˆ‘ä»¬é¼“åŠ±æ‚¨è¯•ç”¨å¹¶åé¦ˆæ„è§ã€‚å¦‚æœ‰éœ€è¦ï¼Œæœªæ¥æˆ‘ä»¬å°†å‘å¸ƒä¿®å¤ç‰ˆæœ¬ã€‚è¯·å‚é˜…[å®˜æ–¹æ–‡æ¡£](https"
#~ "://vllm-kunlun.readthedocs.io/en/v0.7.3-dev)å¼€å¯æ‚¨çš„ä½“éªŒä¹‹æ—…ã€‚"

#~ msgid ""
#~ "This release includes all features "
#~ "landed in the previous release "
#~ "candidates ([v0.7.1rc1](https://github.com/vllm-project"
#~ "/vllm-kunlun/releases/tag/v0.7.1rc1), "
#~ "[v0.7.3rc1](https://github.com/vllm-project/vllm-"
#~ "kunlun/releases/tag/v0.7.3rc1), [v0.7.3rc2](https://github.com"
#~ "/vllm-project/vllm-kunlun/releases/tag/v0.7.3rc2)). And"
#~ " all the features are fully tested"
#~ " and verified. Visit the official doc"
#~ " the get the detail [feature](https"
#~ "://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev/user_guide/suppoted_features.html)"
#~ " and [model](https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev/user_guide/supported_models.html)"
#~ " support matrix."
#~ msgstr ""
#~ "æœ¬æ¬¡å‘å¸ƒåŒ…å«äº†æ‰€æœ‰åœ¨ä¹‹å‰å€™é€‰ç‰ˆæœ¬ä¸­åŠ å…¥çš„åŠŸèƒ½ï¼ˆ[v0.7.1rc1](https://github.com/vllm-"
#~ "project/vllm-"
#~ "kunlun/releases/tag/v0.7.1rc1)ã€[v0.7.3rc1](https://github.com/vllm-"
#~ "project/vllm-"
#~ "kunlun/releases/tag/v0.7.3rc1)ã€[v0.7.3rc2](https://github.com/vllm-"
#~ "project/vllm-"
#~ "kunlun/releases/tag/v0.7.3rc2)ï¼‰ã€‚æ‰€æœ‰åŠŸèƒ½éƒ½ç»è¿‡äº†å…¨é¢æµ‹è¯•å’ŒéªŒè¯ã€‚è¯·è®¿é—®å®˜æ–¹æ–‡æ¡£è·å–è¯¦ç»†çš„[åŠŸèƒ½](https"
#~ "://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev/user_guide/suppoted_features.html)å’Œ[æ¨¡å‹](https"
#~ "://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev/user_guide/supported_models.html)æ”¯æŒçŸ©é˜µã€‚"

#~ msgid ""
#~ "Upgrade CANN to 8.1.RC1 to enable "
#~ "chunked prefill and automatic prefix "
#~ "caching features. You can now enable "
#~ "them now."
#~ msgstr "å°† CANN å‡çº§åˆ° 8.1.RC1 ä»¥å¯ç”¨åˆ†å—é¢„å¡«å……å’Œè‡ªåŠ¨å‰ç¼€ç¼“å­˜åŠŸèƒ½ã€‚æ‚¨ç°åœ¨å¯ä»¥å¯ç”¨è¿™äº›åŠŸèƒ½äº†ã€‚"

#~ msgid ""
#~ "Upgrade PyTorch to 2.5.1. vLLM Kunlun"
#~ " no longer relies on the dev "
#~ "version of torch-xpu now. Now "
#~ "users don't need to install the "
#~ "torch-xpu by hand. The 2.5.1 version"
#~ " of torch-xpu will be installed "
#~ "automatically. [#662](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/662)"
#~ msgstr ""
#~ "å‡çº§ PyTorch è‡³ 2.5.1ã€‚vLLM Kunlun ç°åœ¨ä¸å†ä¾èµ–äº"
#~ " torch-xpu çš„å¼€å‘ç‰ˆæœ¬ã€‚ç”¨æˆ·ç°åœ¨æ— éœ€æ‰‹åŠ¨å®‰è£… torch-xpuï¼Œ2.5.1"
#~ " ç‰ˆæœ¬çš„ torch-xpu ä¼šè¢«è‡ªåŠ¨å®‰è£…ã€‚[#662](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/662)"

#~ msgid ""
#~ "Integrate MindIE Turbo into vLLM Kunlun"
#~ " to improve DeepSeek V3/R1, Qwen 2"
#~ " series performance. [#708](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/708)"
#~ msgstr ""
#~ "å°† MindIE Turbo é›†æˆåˆ° vLLM Kunlun ä»¥æå‡"
#~ " DeepSeek V3/R1ã€Qwen 2 "
#~ "ç³»åˆ—çš„æ€§èƒ½ã€‚[#708](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/708)"

#~ msgid ""
#~ "LoRAã€Multi-LoRA And Dynamic Serving is"
#~ " supported now. The performance will "
#~ "be improved in the next release. "
#~ "Please follow the official doc for "
#~ "more usage information. Thanks for the"
#~ " contribution from China Merchants Bank."
#~ " [#700](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/700)"
#~ msgstr ""
#~ "ç°åœ¨å·²ç»æ”¯æŒ LoRAã€å¤šLoRA "
#~ "å’ŒåŠ¨æ€æœåŠ¡ã€‚ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­æ€§èƒ½å°†ä¼šæå‡ã€‚è¯·å‚é˜…å®˜æ–¹æ–‡æ¡£ä»¥è·å–æ›´å¤šç”¨æ³•ä¿¡æ¯ã€‚æ„Ÿè°¢æ‹›å•†é“¶è¡Œçš„è´¡çŒ®ã€‚[#700](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/700)"

#~ msgid ""
#~ "The performance of Qwen2 vl and "
#~ "Qwen2.5 vl is improved. "
#~ "[#702](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/702)"
#~ msgstr ""
#~ "Qwen2 vl å’Œ Qwen2.5 vl çš„æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚ "
#~ "[#702](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/702)"

#~ msgid ""
#~ "The performance of `apply_penalties` and "
#~ "`topKtopP` ops are improved. "
#~ "[#525](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/525)"
#~ msgstr ""
#~ "`apply_penalties` å’Œ `topKtopP` æ“ä½œçš„æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚ "
#~ "[#525](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/525)"

#~ msgid ""
#~ "Fixed a issue that may lead CPU"
#~ " memory leak. [#691](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/691) [#712](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/712)"
#~ msgstr ""
#~ "ä¿®å¤äº†å¯èƒ½å¯¼è‡´CPUå†…å­˜æ³„æ¼çš„é—®é¢˜ã€‚ [#691](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/691) [#712](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/712)"

#~ msgid ""
#~ "A new environment `SOC_VERSION` is "
#~ "added. If you hit any soc "
#~ "detection error when building with "
#~ "custom ops enabled, please set "
#~ "`SOC_VERSION` to a suitable value. "
#~ "[#606](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/606)"
#~ msgstr ""
#~ "æ–°å¢äº†ä¸€ä¸ªç¯å¢ƒå˜é‡ `SOC_VERSION`ã€‚å¦‚æœåœ¨å¯ç”¨è‡ªå®šä¹‰ç®—å­æ—¶æ„å»ºè¿‡ç¨‹ä¸­é‡åˆ° soc "
#~ "æ£€æµ‹é”™è¯¯ï¼Œè¯·å°† `SOC_VERSION` è®¾ç½®ä¸ºåˆé€‚çš„å€¼ã€‚[#606](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/606)"

#~ msgid ""
#~ "openEuler container image supported with "
#~ "v0.7.3-openeuler tag. [#665](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/665)"
#~ msgstr ""
#~ "openEuler å®¹å™¨é•œåƒå·²æ”¯æŒ v0.7.3-openeuler "
#~ "æ ‡ç­¾ã€‚[#665](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/665)"

#~ msgid ""
#~ "Prefix cache feature works on V1 "
#~ "engine now. [#559](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/559)"
#~ msgstr ""
#~ "å‰ç¼€ç¼“å­˜åŠŸèƒ½ç°åœ¨å·²åœ¨ V1 å¼•æ“ä¸Šå·¥ä½œã€‚[#559](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/559)"

#~ msgid "v0.8.5rc1 - 2025.05.06"
#~ msgstr "v0.8.5rc1 - 2025.05.06"

#~ msgid ""
#~ "This is the 1st release candidate "
#~ "of v0.8.5 for vllm-kunlun. Please "
#~ "follow the [official doc](https://vllm-"
#~ "kunlun.readthedocs.io/en/) to start the "
#~ "journey. Now you can enable V1 "
#~ "egnine by setting the environment "
#~ "variable `VLLM_USE_V1=1`, see the feature "
#~ "support status of vLLM Kunlun in "
#~ "[here](https://vllm-"
#~ "kunlun.readthedocs.io/en/latest/user_guide/support_matrix/supported_features.html)."
#~ msgstr ""
#~ "è¿™æ˜¯ vllm-kunlun v0.8.5 "
#~ "çš„ç¬¬ä¸€ä¸ªå€™é€‰å‘å¸ƒç‰ˆæœ¬ã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-"
#~ "kunlun.readthedocs.io/en/)å¼€å§‹ä½¿ç”¨ã€‚ç°åœ¨ï¼Œæ‚¨å¯ä»¥é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡ `VLLM_USE_V1=1`"
#~ " å¯ç”¨ V1 å¼•æ“ã€‚å…³äº vLLM Kunlun "
#~ "çš„ç‰¹æ€§æ”¯æŒæƒ…å†µï¼Œè¯·å‚è§[è¿™é‡Œ](https://vllm-"
#~ "kunlun.readthedocs.io/en/latest/user_guide/support_matrix/supported_features.html)ã€‚"

#~ msgid ""
#~ "Upgrade CANN version to 8.1.RC1 to "
#~ "support chunked prefill and automatic "
#~ "prefix caching (`--enable_prefix_caching`) when "
#~ "V1 is enabled [#747](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/747)"
#~ msgstr ""
#~ "å°† CANN ç‰ˆæœ¬å‡çº§åˆ° 8.1.RC1ï¼Œä»¥æ”¯æŒåœ¨å¯ç”¨ V1 "
#~ "æ—¶çš„åˆ†å—é¢„å¡«å……å’Œè‡ªåŠ¨å‰ç¼€ç¼“å­˜ï¼ˆ`--enable_prefix_caching`ï¼‰[#747](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/747)"

#~ msgid ""
#~ "Optimize Qwen2 VL and Qwen 2.5 VL"
#~ " [#701](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/701)"
#~ msgstr ""
#~ "ä¼˜åŒ– Qwen2 VL å’Œ Qwen 2.5 VL "
#~ "[#701](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/701)"

#~ msgid ""
#~ "Improve Deepseek V3 eager mode and "
#~ "graph mode performance, now you can "
#~ "use --additional_config={'enable_graph_mode': True} "
#~ "to enable graph mode. "
#~ "[#598](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/598) [#719](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/719)"
#~ msgstr ""
#~ "æ”¹è¿›äº† Deepseek V3 çš„ eager æ¨¡å¼å’Œå›¾æ¨¡å¼æ€§èƒ½ï¼Œç°åœ¨ä½ å¯ä»¥ä½¿ç”¨"
#~ " --additional_config={'enable_graph_mode': True} "
#~ "æ¥å¯ç”¨å›¾æ¨¡å¼ã€‚[#598](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/598) [#719](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/719)"

#~ msgid ""
#~ "Upgrade vLLM to 0.8.5.post1 "
#~ "[#715](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/715)"
#~ msgstr ""
#~ "å°† vLLM å‡çº§åˆ° 0.8.5.post1 "
#~ "[#715](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/715)"

#~ msgid ""
#~ "Fix early return in "
#~ "CustomDeepseekV2MoE.forward during profile_run "
#~ "[#682](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/682)"
#~ msgstr ""
#~ "ä¿®å¤åœ¨ profile_run æœŸé—´ CustomDeepseekV2MoE.forward "
#~ "è¿‡æ—©è¿”å›çš„é—®é¢˜ [#682](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/682)"

#~ msgid ""
#~ "Adapts for new quant model generated "
#~ "by modelslim [#719](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/719)"
#~ msgstr ""
#~ "é€‚é…ç”± modelslim ç”Ÿæˆçš„æ–°é‡åŒ–æ¨¡å‹ [#719](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/719)"

#~ msgid ""
#~ "Initial support on P2P Disaggregated "
#~ "Prefill based on llm_datadist "
#~ "[#694](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/694)"
#~ msgstr ""
#~ "åŸºäº llm_datadist çš„ P2P åˆ†å¸ƒå¼ Prefill "
#~ "åˆæ­¥æ”¯æŒ [#694](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/694)"

#~ msgid ""
#~ "Use `/vllm-workspace` as code path "
#~ "and include `.git` in container image"
#~ " to fix issue when start vllm "
#~ "under `/workspace` [#726](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/726)"
#~ msgstr ""
#~ "ä½¿ç”¨ `/vllm-workspace` ä½œä¸ºä»£ç è·¯å¾„ï¼Œå¹¶åœ¨å®¹å™¨é•œåƒä¸­åŒ…å« `.git`"
#~ " ï¼Œä»¥ä¿®å¤åœ¨ `/workspace` ä¸‹å¯åŠ¨ vllm æ—¶çš„é—®é¢˜ "
#~ "[#726](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/726)"

#~ msgid ""
#~ "Optimize XPU memory usage to make "
#~ "DeepSeek R1 W8A8 32K model len "
#~ "work. [#728](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/728)"
#~ msgstr ""
#~ "ä¼˜åŒ–XPUå†…å­˜ä½¿ç”¨ï¼Œä»¥ä½¿ DeepSeek R1 W8A8 32K "
#~ "æ¨¡å‹é•¿åº¦èƒ½å¤Ÿè¿è¡Œã€‚[#728](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/728)"

#~ msgid ""
#~ "Fix `PYTHON_INCLUDE_PATH` typo in setup.py "
#~ "[#762](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/762)"
#~ msgstr ""
#~ "ä¿®å¤ setup.py ä¸­çš„ `PYTHON_INCLUDE_PATH` æ‹¼å†™é”™è¯¯ "
#~ "[#762](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/762)"

#~ msgid ""
#~ "Add Qwen3-0.6B test [#717](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/717)"
#~ msgstr ""
#~ "æ·»åŠ  Qwen3-0.6B æµ‹è¯• [#717](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/717)"

#~ msgid ""
#~ "Add nightly CI [#668](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/668)"
#~ msgstr "æ·»åŠ æ¯æ™šæŒç»­é›†æˆ [#668](https://github.com/vllm-project/vllm-kunlun/pull/668)"

#~ msgid ""
#~ "Add accuracy test report "
#~ "[#542](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/542)"
#~ msgstr "æ·»åŠ å‡†ç¡®æ€§æµ‹è¯•æŠ¥å‘Š [#542](https://github.com/vllm-project/vllm-kunlun/pull/542)"

#~ msgid "v0.8.4rc2 - 2025.04.29"
#~ msgstr "v0.8.4rc2 - 2025.04.29"

#~ msgid ""
#~ "This is the second release candidate "
#~ "of v0.8.4 for vllm-kunlun. Please "
#~ "follow the [official doc](https://vllm-"
#~ "kunlun.readthedocs.io/en/) to start the "
#~ "journey. Some experimental features are "
#~ "included in this version, such as "
#~ "W8A8 quantization and EP/DP support. "
#~ "We'll make them stable enough in "
#~ "the next release."
#~ msgstr ""
#~ "è¿™æ˜¯ vllm-kunlun çš„ v0.8.4 "
#~ "ç¬¬äºŒä¸ªå€™é€‰ç‰ˆæœ¬ã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-"
#~ "kunlun.readthedocs.io/en/)å¼€å§‹ä½¿ç”¨ã€‚æœ¬ç‰ˆæœ¬åŒ…å«äº†ä¸€äº›å®éªŒæ€§åŠŸèƒ½ï¼Œå¦‚ W8A8 é‡åŒ–å’Œ"
#~ " EP/DP æ”¯æŒã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­ä½¿è¿™äº›åŠŸèƒ½æ›´åŠ ç¨³å®šã€‚"

#~ msgid ""
#~ "Qwen3 and Qwen3MOE is supported now. "
#~ "Please follow the [official doc](https"
#~ "://vllm-kunlun.readthedocs.io/en/latest/tutorials/single_npu.html)"
#~ " to run the quick demo. "
#~ "[#709](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/709)"
#~ msgstr ""
#~ "ç°åœ¨å·²æ”¯æŒ Qwen3 å’Œ Qwen3MOEã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-"
#~ "kunlun.readthedocs.io/en/latest/tutorials/single_npu.html)è¿è¡Œå¿«é€Ÿæ¼”ç¤ºã€‚[#709](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/709)"

#~ msgid ""
#~ "Kunlun W8A8 quantization method is "
#~ "supported now. Please take the [official"
#~ " doc](https://vllm-"
#~ "kunlun.readthedocs.io/en/latest/tutorials/multi_npu_quantization.html)"
#~ " for example. Any [feedback](https://github.com"
#~ "/vllm-project/vllm-kunlun/issues/619) is welcome."
#~ " [#580](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/580)"
#~ msgstr ""
#~ "ç°åœ¨æ”¯æŒ Kunlun W8A8 é‡åŒ–æ–¹æ³•ã€‚è¯·å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://vllm-"
#~ "kunlun.readthedocs.io/en/latest/tutorials/multi_npu_quantization.html)"
#~ " ç¤ºä¾‹ã€‚æ¬¢è¿æä¾›ä»»ä½•[åé¦ˆ](https://github.com/vllm-project/vllm-"
#~ "kunlun/issues/619)ã€‚[#580](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/580)"

#~ msgid ""
#~ "DeepSeek V3/R1 works with DP, TP "
#~ "and MTP now. Please note that it's"
#~ " still in experimental status. Let us"
#~ " know if you hit any problem. "
#~ "[#429](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/429) [#585](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/585)  [#626](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/626) [#636](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/636) "
#~ "[#671](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/671)"
#~ msgstr ""
#~ "DeepSeek V3/R1 ç°åœ¨å·²ç»æ”¯æŒ DPã€TP å’Œ "
#~ "MTPã€‚è¯·æ³¨æ„ï¼Œç›®å‰ä»å¤„äºå®éªŒé˜¶æ®µã€‚å¦‚æœé‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·å‘ŠçŸ¥æˆ‘ä»¬ã€‚ [#429](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/429) "
#~ "[#585](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/585) [#626](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/626) [#636](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/636) [#671](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/671)"

#~ msgid ""
#~ "ACLGraph feature is supported with V1"
#~ " engine now. It's disabled by default"
#~ " because this feature rely on CANN"
#~ " 8.1 release. We'll make it available"
#~ " by default in the next release "
#~ "[#426](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/426)"
#~ msgstr ""
#~ "ACLGraph ç‰¹æ€§ç°åœ¨å·²è¢« V1 å¼•æ“æ”¯æŒã€‚å®ƒé»˜è®¤æ˜¯ç¦ç”¨çš„ï¼Œå› ä¸ºè¯¥ç‰¹æ€§ä¾èµ–äº CANN"
#~ " 8.1 ç‰ˆæœ¬ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­é»˜è®¤å¯ç”¨æ­¤ç‰¹æ€§ [#426](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/426)ã€‚"

#~ msgid ""
#~ "Upgrade PyTorch to 2.5.1. vLLM Kunlun"
#~ " no longer relies on the dev "
#~ "version of torch-xpu now. Now "
#~ "users don't need to install the "
#~ "torch-xpu by hand. The 2.5.1 version"
#~ " of torch-xpu will be installed "
#~ "automatically. [#661](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/661)"
#~ msgstr ""
#~ "å‡çº§ PyTorch è‡³ 2.5.1ã€‚vLLM Kunlun ç°åœ¨ä¸å†ä¾èµ–"
#~ " dev ç‰ˆæœ¬çš„ torch-xpuï¼Œç”¨æˆ·æ— éœ€æ‰‹åŠ¨å®‰è£… torch-xpu"
#~ "ã€‚torch-xpu çš„ 2.5.1 "
#~ "ç‰ˆæœ¬å°†ä¼šè‡ªåŠ¨å®‰è£…ã€‚[#661](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/661)"

#~ msgid ""
#~ "MiniCPM model works now. "
#~ "[#645](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/645)"
#~ msgstr ""
#~ "MiniCPM æ¨¡å‹ç°åœ¨å¯ä»¥ä½¿ç”¨äº†ã€‚[#645](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/645)"

#~ msgid ""
#~ "openEuler container image supported with "
#~ "`v0.8.4-openeuler` tag and customs Ops "
#~ "build is enabled by default for "
#~ "openEuler OS. [#689](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/689)"
#~ msgstr ""
#~ "openEuler å®¹å™¨é•œåƒå·²æ”¯æŒ `v0.8.4-openeuler` æ ‡ç­¾ï¼Œå¹¶ä¸” "
#~ "openEuler æ“ä½œç³»ç»Ÿé»˜è®¤å¯ç”¨äº†è‡ªå®šä¹‰ Ops "
#~ "æ„å»ºã€‚[#689](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/689)"

#~ msgid ""
#~ "Fix ModuleNotFoundError bug to make Lora"
#~ " work [#600](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/600)"
#~ msgstr ""
#~ "ä¿®å¤ ModuleNotFoundError é”™è¯¯ä»¥ä½¿ Lora æ­£å¸¸å·¥ä½œ "
#~ "[#600](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/600)"

#~ msgid ""
#~ "Add \"Using EvalScope evaluation\" doc "
#~ "[#611](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/611)"
#~ msgstr ""
#~ "æ·»åŠ äº†â€œä½¿ç”¨ EvalScope è¯„ä¼°â€æ–‡æ¡£ [#611](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/611)"

#~ msgid ""
#~ "Add a `VLLM_VERSION` environment to make"
#~ " vLLM version configurable to help "
#~ "developer set correct vLLM version if"
#~ " the code of vLLM is changed by"
#~ " hand locally. [#651](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/651)"
#~ msgstr ""
#~ "æ–°å¢äº†ä¸€ä¸ª `VLLM_VERSION` ç¯å¢ƒå˜é‡ï¼Œä½¿ vLLM "
#~ "ç‰ˆæœ¬å¯ä»¥é…ç½®ï¼Œå¸®åŠ©å¼€å‘è€…åœ¨æœ¬åœ°æ‰‹åŠ¨ä¿®æ”¹ vLLM ä»£ç åï¼Œè®¾ç½®æ­£ç¡®çš„ vLLM "
#~ "ç‰ˆæœ¬ã€‚[#651](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/651)"

#~ msgid "v0.8.4rc1 - 2025.04.18"
#~ msgstr "v0.8.4rc1 - 2025.04.18"

#~ msgid ""
#~ "This is the first release candidate "
#~ "of v0.8.4 for vllm-kunlun. Please "
#~ "follow the [official doc](https://vllm-"
#~ "kunlun.readthedocs.io/en/) to start the "
#~ "journey. From this version, vllm-kunlun"
#~ " will follow the newest version of"
#~ " vllm and release every two weeks."
#~ " For example, if vllm releases v0.8.5"
#~ " in the next two weeks, vllm-"
#~ "kunlun will release v0.8.5rc1 instead of"
#~ " v0.8.4rc2. Please find the detail "
#~ "from the [official documentation](https://vllm-"
#~ "kunlun.readthedocs.io/en/latest/community/versioning_policy.html"
#~ "#release-window)."
#~ msgstr ""
#~ "è¿™æ˜¯ vllm-kunlun v0.8.4 "
#~ "çš„ç¬¬ä¸€ä¸ªå€™é€‰å‘å¸ƒç‰ˆæœ¬ã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-"
#~ "kunlun.readthedocs.io/en/)å¼€å§‹ä½¿ç”¨ã€‚æœ¬ç‰ˆæœ¬èµ·ï¼Œvllm-kunlun å°†è·Ÿéš "
#~ "vllm çš„æœ€æ–°ç‰ˆæœ¬å¹¶æ¯ä¸¤å‘¨å‘å¸ƒä¸€æ¬¡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ vllm åœ¨æ¥ä¸‹æ¥çš„ä¸¤å‘¨å†…å‘å¸ƒ "
#~ "v0.8.5ï¼Œvllm-kunlun å°†å‘å¸ƒ v0.8.5rc1ï¼Œè€Œä¸æ˜¯ "
#~ "v0.8.4rc2ã€‚è¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://vllm-"
#~ "kunlun.readthedocs.io/en/latest/community/versioning_policy.html"
#~ "#release-window)ã€‚"

#~ msgid ""
#~ "vLLM V1 engine experimental support is"
#~ " included in this version. You can"
#~ " visit [official "
#~ "guide](https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html)"
#~ " to get more detail. By default, "
#~ "vLLM will fallback to V0 if V1 "
#~ "doesn't work, please set `VLLM_USE_V1=1` "
#~ "environment if you want to use V1"
#~ " forcely."
#~ msgstr ""
#~ "æœ¬ç‰ˆæœ¬åŒ…å«äº†å¯¹ vLLM V1 "
#~ "å¼•æ“çš„å®éªŒæ€§æ”¯æŒã€‚ä½ å¯ä»¥è®¿é—®[å®˜æ–¹æŒ‡å—](https://docs.vllm.ai/en/latest/getting_started/v1_user_guide.html)è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœ"
#~ " V1 ä¸å¯ç”¨ï¼ŒvLLM ä¼šè‡ªåŠ¨å›é€€åˆ° V0ã€‚å¦‚æœä½ æƒ³å¼ºåˆ¶ä½¿ç”¨ V1ï¼Œè¯·è®¾ç½® "
#~ "`VLLM_USE_V1=1` ç¯å¢ƒå˜é‡ã€‚"

#~ msgid ""
#~ "LoRAã€Multi-LoRA And Dynamic Serving is"
#~ " supported now. The performance will "
#~ "be improved in the next release. "
#~ "Please follow the [official "
#~ "doc](https://docs.vllm.ai/en/latest/features/lora.html) for "
#~ "more usage information. Thanks for the"
#~ " contribution from China Merchants Bank."
#~ " [#521](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/521)."
#~ msgstr ""
#~ "ç°åœ¨å·²æ”¯æŒ LoRAã€Multi-LoRA "
#~ "å’ŒåŠ¨æ€æœåŠ¡ã€‚æ€§èƒ½å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­å¾—åˆ°æå‡ã€‚è¯·å‚é˜…[å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/en/latest/features/lora.html)è·å–æ›´å¤šä½¿ç”¨ä¿¡æ¯ã€‚æ„Ÿè°¢æ‹›å•†é“¶è¡Œçš„è´¡çŒ®ã€‚[#521](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/521)ã€‚"

#~ msgid ""
#~ "Sleep Mode feature is supported. "
#~ "Currently it's only work on V0 "
#~ "engine. V1 engine support will come "
#~ "soon. [#513](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/513)"
#~ msgstr ""
#~ "å·²æ”¯æŒä¼‘çœ æ¨¡å¼åŠŸèƒ½ã€‚ç›®å‰å®ƒåªåœ¨V0å¼•æ“ä¸Šæœ‰æ•ˆï¼ŒV1å¼•æ“çš„æ”¯æŒå³å°†åˆ°æ¥ã€‚[#513](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/513)"

#~ msgid ""
#~ "The Kunlun scheduler is added for "
#~ "V1 engine. This scheduler is more "
#~ "affinity with Kunlun hardware. More "
#~ "scheduler policy will be added in "
#~ "the future. [#543](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/543)"
#~ msgstr ""
#~ "ä¸ºV1å¼•æ“æ–°å¢äº†Kunlunè°ƒåº¦å™¨ã€‚è¯¥è°ƒåº¦å™¨ä¸Kunlunç¡¬ä»¶æ›´åŠ é€‚é…ã€‚æœªæ¥è¿˜å°†æ·»åŠ æ›´å¤šè°ƒåº¦ç­–ç•¥ã€‚ "
#~ "[#543](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/543)"

#~ msgid ""
#~ "Disaggregated Prefill feature is supported."
#~ " Currently only 1P1D works. NPND is"
#~ " under design by vllm team. vllm-"
#~ "kunlun will support it once it's "
#~ "ready from vLLM. Follow the [official"
#~ " "
#~ "guide](https://docs.vllm.ai/en/latest/features/disagg_prefill.html)"
#~ " to use. [#432](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/432)"
#~ msgstr ""
#~ "æ”¯æŒåˆ†ç¦»å¼é¢„å¡«å……ï¼ˆDisaggregated "
#~ "Prefillï¼‰åŠŸèƒ½ã€‚ç›®å‰ä»…æ”¯æŒ1P1Dï¼ŒNPNDæ­£åœ¨ç”±vllmå›¢é˜Ÿè®¾è®¡ä¸­ã€‚ä¸€æ—¦vLLMæ”¯æŒï¼Œvllm-"
#~ "kunlunå°†ä¼šæ”¯æŒã€‚è¯·æŒ‰ç…§[å®˜æ–¹æŒ‡å—](https://docs.vllm.ai/en/latest/features/disagg_prefill.html)ä½¿ç”¨ã€‚[#432](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/432)"

#~ msgid ""
#~ "Spec decode feature works now. Currently"
#~ " it's only work on V0 engine. "
#~ "V1 engine support will come soon. "
#~ "[#500](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/500)"
#~ msgstr ""
#~ "Spec è§£ç åŠŸèƒ½ç°åœ¨å¯ä»¥ä½¿ç”¨ã€‚ç›®å‰å®ƒåªåœ¨ V0 å¼•æ“ä¸Šå·¥ä½œï¼Œå¯¹ V1 "
#~ "å¼•æ“çš„æ”¯æŒå³å°†åˆ°æ¥ã€‚[#500](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/500)"

#~ msgid ""
#~ "Structured output feature works now on"
#~ " V1 Engine. Currently it only "
#~ "supports xgrammar backend while using "
#~ "guidance backend may get some errors."
#~ " [#555](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/555)"
#~ msgstr ""
#~ "ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½ç°åœ¨å·²åœ¨V1å¼•æ“ä¸Šç”Ÿæ•ˆã€‚ç›®å‰ä»…æ”¯æŒxgrammaråç«¯ï¼Œä½¿ç”¨guidanceåç«¯å¯èƒ½ä¼šå‡ºç°ä¸€äº›é”™è¯¯ã€‚[#555](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/555)"

#~ msgid ""
#~ "A new communicator `pyhccl` is added."
#~ " It's used for call CANN HCCL "
#~ "library directly instead of using "
#~ "`torch.distribute`. More usage of it "
#~ "will be added in the next release"
#~ " [#503](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/503)"
#~ msgstr ""
#~ "æ–°å¢äº†ä¸€ä¸ªé€šä¿¡å™¨ `pyhccl`ã€‚å®ƒç”¨äºç›´æ¥è°ƒç”¨ CANN HCCL åº“ï¼Œè€Œä¸æ˜¯ä½¿ç”¨"
#~ " `torch.distribute`ã€‚å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­æ·»åŠ æ›´å¤šç”¨æ³• [#503](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/503)ã€‚"

#~ msgid ""
#~ "The custom ops build is enabled by"
#~ " default. You should install the "
#~ "packages like `gcc`, `cmake` first to"
#~ " build `vllm-kunlun` from source. Set"
#~ " `COMPILE_CUSTOM_KERNELS=0` environment to "
#~ "disable the compilation if you don't "
#~ "need it. [#466](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/466)"
#~ msgstr ""
#~ "è‡ªå®šä¹‰ç®—å­çš„æ„å»ºé»˜è®¤æ˜¯å¯ç”¨çš„ã€‚ä½ åº”è¯¥å…ˆå®‰è£…å¦‚ `gcc`ã€`cmake` ç­‰åŒ…ä»¥ä¾¿ä»æºç ç¼–è¯‘ "
#~ "`vllm-kunlun`ã€‚å¦‚æœä¸éœ€è¦è‡ªå®šä¹‰ç®—å­çš„ç¼–è¯‘ï¼Œå¯ä»¥è®¾ç½®ç¯å¢ƒå˜é‡ "
#~ "`COMPILE_CUSTOM_KERNELS=0` æ¥ç¦ç”¨ç¼–è¯‘ã€‚ "
#~ "[#466](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/466)"

#~ msgid ""
#~ "The custom op `rotay embedding` is "
#~ "enabled by default now to improve "
#~ "the performance. [#555](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/555)"
#~ msgstr ""
#~ "è‡ªå®šä¹‰ç®—å­ `rotay embedding` "
#~ "ç°åœ¨å·²é»˜è®¤å¯ç”¨ï¼Œä»¥æå‡æ€§èƒ½ã€‚[#555](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/555)"

#~ msgid "v0.7.3rc2 - 2025.03.29"
#~ msgstr "v0.7.3rc2 - 2025.03.29"

#~ msgid ""
#~ "This is 2nd release candidate of "
#~ "v0.7.3 for vllm-kunlun. Please follow"
#~ " the [official doc](https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev) to start the"
#~ " journey."
#~ msgstr ""
#~ "è¿™æ˜¯ vllm-kunlun v0.7.3 "
#~ "çš„ç¬¬äºŒä¸ªå€™é€‰å‘å¸ƒç‰ˆæœ¬ã€‚è¯·æ ¹æ®[å®˜æ–¹æ–‡æ¡£](https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev)å¼€å§‹ä½¿ç”¨ã€‚"

#~ msgid ""
#~ "Quickstart with container: https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev/quick_start.html"
#~ msgstr ""
#~ "å®¹å™¨å¿«é€Ÿå…¥é—¨ï¼š https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev/quick_start.html"

#~ msgid ""
#~ "Installation: https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev/installation.html"
#~ msgstr "å®‰è£…: https://vllm-kunlun.readthedocs.io/en/v0.7.3-dev/installation.html"

#~ msgid ""
#~ "Add Kunlun Custom Ops framewrok. "
#~ "Developers now can write customs ops "
#~ "using KunlunC. An example ops "
#~ "`rotary_embedding` is added. More tutorials"
#~ " will come soon. The Custom Ops "
#~ "compilation is disabled by default when"
#~ " installing vllm-kunlun. Set "
#~ "`COMPILE_CUSTOM_KERNELS=1` to enable it.  "
#~ "[#371](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/371)"
#~ msgstr ""
#~ "æ–°å¢äº†Kunlunè‡ªå®šä¹‰ç®—å­æ¡†æ¶ã€‚å¼€å‘è€…ç°åœ¨å¯ä»¥ä½¿ç”¨KunlunCç¼–å†™è‡ªå®šä¹‰ç®—å­ã€‚æ–°å¢äº†ä¸€ä¸ªç¤ºä¾‹ç®—å­ "
#~ "`rotary_embedding` ã€‚æ›´å¤šæ•™ç¨‹å³å°†å‘å¸ƒã€‚å®‰è£…vllm-"
#~ "kunlunæ—¶ï¼Œè‡ªå®šä¹‰ç®—å­çš„ç¼–è¯‘é»˜è®¤æ˜¯å…³é—­çš„ã€‚å¯é€šè¿‡è®¾ç½® `COMPILE_CUSTOM_KERNELS=1` "
#~ "å¯ç”¨ã€‚[#371](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/371)"

#~ msgid ""
#~ "V1 engine is basic supported in "
#~ "this release. The full support will "
#~ "be done in 0.8.X release. If you"
#~ " hit any issue or have any "
#~ "requirement of V1 engine. Please tell"
#~ " us [here](https://github.com/vllm-project/vllm-"
#~ "kunlun/issues/414). [#376](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/376)"
#~ msgstr ""
#~ "æœ¬ç‰ˆæœ¬å¯¹ V1 å¼•æ“æä¾›äº†åŸºç¡€æ”¯æŒï¼Œå…¨é¢æ”¯æŒå°†åœ¨ 0.8.X "
#~ "ç‰ˆæœ¬ä¸­å®Œæˆã€‚å¦‚æœæ‚¨é‡åˆ°ä»»ä½•é—®é¢˜æˆ–æœ‰ V1 å¼•æ“çš„ç›¸å…³éœ€æ±‚ï¼Œè¯·åœ¨[è¿™é‡Œ](https://github.com"
#~ "/vllm-project/vllm-"
#~ "kunlun/issues/414)å‘Šè¯‰æˆ‘ä»¬ã€‚[#376](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/376)"

#~ msgid ""
#~ "Prefix cache feature works now. You "
#~ "can set `enable_prefix_caching=True` to enable"
#~ " it. [#282](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/282)"
#~ msgstr ""
#~ "å‰ç¼€ç¼“å­˜åŠŸèƒ½ç°åœ¨å·²ç»å¯ç”¨ã€‚ä½ å¯ä»¥é€šè¿‡è®¾ç½® `enable_prefix_caching=True` "
#~ "æ¥å¯ç”¨è¯¥åŠŸèƒ½ã€‚[#282](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/282)"

#~ msgid ""
#~ "Bump torch_npu version to dev20250320.3 "
#~ "to improve accuracy to fix `!!!` "
#~ "output problem. [#406](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/406)"
#~ msgstr ""
#~ "å°† torch_npu ç‰ˆæœ¬å‡çº§åˆ° dev20250320.3 ä»¥æå‡ç²¾åº¦ï¼Œä¿®å¤ "
#~ "`!!!` è¾“å‡ºé—®é¢˜ã€‚[#406](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/406)"

#~ msgid ""
#~ "The performance of Qwen2-vl is improved"
#~ " by optimizing patch embedding (Conv3D)."
#~ " [#398](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/398)"
#~ msgstr ""
#~ "é€šè¿‡ä¼˜åŒ– patch embeddingï¼ˆConv3Dï¼‰ï¼ŒQwen2-vl "
#~ "çš„æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚[#398](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/398)"

#~ msgid ""
#~ "Fixed a bug to make sure multi "
#~ "step scheduler feature work. "
#~ "[#349](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/349)"
#~ msgstr ""
#~ "ä¿®å¤äº†ä¸€ä¸ªé”™è¯¯ï¼Œä»¥ç¡®ä¿å¤šæ­¥è°ƒåº¦å™¨åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚[#349](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/349)"

#~ msgid ""
#~ "Fixed a bug to make prefix cache"
#~ " feature works with correct accuracy. "
#~ "[#424](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/424)"
#~ msgstr ""
#~ "ä¿®å¤äº†ä¸€ä¸ª bugï¼Œä½¿å‰ç¼€ç¼“å­˜åŠŸèƒ½èƒ½å¤Ÿä»¥æ­£ç¡®çš„å‡†ç¡®æ€§è¿è¡Œã€‚[#424](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/424)"

#~ msgid "v0.7.3rc1 - 2025.03.14"
#~ msgstr "v0.7.3rc1 - 2025.03.14"

#~ msgid ""
#~ "ğŸ‰ Hello, World! This is the first"
#~ " release candidate of v0.7.3 for "
#~ "vllm-kunlun. Please follow the [official"
#~ " doc](https://vllm-kunlun.readthedocs.io/en/v0.7.3-dev) "
#~ "to start the journey."
#~ msgstr ""
#~ "ğŸ‰ ä½ å¥½ï¼Œä¸–ç•Œï¼è¿™æ˜¯ vllm-kunlun v0.7.3 "
#~ "çš„ç¬¬ä¸€ä¸ªå€™é€‰å‘å¸ƒç‰ˆæœ¬ã€‚è¯·æŒ‰ç…§[å®˜æ–¹æ–‡æ¡£](https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev)å¼€å§‹ä½ çš„æ—…ç¨‹ã€‚"

#~ msgid ""
#~ "DeepSeek V3/R1 works well now. Read "
#~ "the [official guide](https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev/tutorials/multi_node.html) "
#~ "to start! [#242](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/242)"
#~ msgstr ""
#~ "DeepSeek V3/R1 ç°åœ¨è¿è¡Œè‰¯å¥½ã€‚è¯·é˜…è¯»[å®˜æ–¹æŒ‡å—](https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev/tutorials/multi_node.html)å¼€å§‹ï¼[#242](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/242)"

#~ msgid ""
#~ "Speculative decoding feature is supported. "
#~ "[#252](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/252)"
#~ msgstr "å·²æ”¯æŒçŒœæµ‹æ€§è§£ç åŠŸèƒ½ã€‚[#252](https://github.com/vllm-project/vllm-kunlun/pull/252)"

#~ msgid ""
#~ "Multi step scheduler feature is "
#~ "supported. [#300](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/300)"
#~ msgstr "å·²æ”¯æŒå¤šæ­¥è°ƒåº¦å™¨åŠŸèƒ½ã€‚[#300](https://github.com/vllm-project/vllm-kunlun/pull/300)"

#~ msgid ""
#~ "Bump torch_npu version to dev20250308.3 "
#~ "to improve `_exponential` accuracy"
#~ msgstr "å°† torch_npu ç‰ˆæœ¬å‡çº§åˆ° dev20250308.3ï¼Œä»¥æå‡ `_exponential` çš„ç²¾åº¦"

#~ msgid ""
#~ "Added initial support for pooling "
#~ "models. Bert based model, such as "
#~ "`BAAI/bge-base-en-v1.5` and `BAAI/bge-"
#~ "reranker-v2-m3` works now. [#229](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/229)"
#~ msgstr ""
#~ "æ–°å¢äº†å¯¹æ± åŒ–æ¨¡å‹çš„åˆæ­¥æ”¯æŒã€‚ç°åœ¨æ”¯æŒ Bert åŸºç¡€æ¨¡å‹ï¼Œå¦‚ `BAAI/bge-"
#~ "base-en-v1.5` å’Œ `BAAI/bge-reranker-v2-m3`ã€‚ "
#~ "[#229](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/229)"

#~ msgid ""
#~ "The performance of Qwen2-VL is improved."
#~ " [#241](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/241)"
#~ msgstr ""
#~ "Qwen2-VL çš„æ€§èƒ½å¾—åˆ°äº†æå‡ã€‚[#241](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/241)"

#~ msgid ""
#~ "MiniCPM is now supported "
#~ "[#164](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/164)"
#~ msgstr ""
#~ "MiniCPM ç°åœ¨å·²è¢«æ”¯æŒ [#164](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/164)"

#~ msgid ""
#~ "Support MTP(Multi-Token Prediction) for "
#~ "DeepSeek V3/R1 [#236](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/236)"
#~ msgstr ""
#~ "ä¸º DeepSeek V3/R1 æ”¯æŒ MTPï¼ˆå¤šæ ‡è®°é¢„æµ‹ï¼‰ "
#~ "[#236](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/236)"

#~ msgid ""
#~ "[Docs] Added more model tutorials, "
#~ "include DeepSeek, QwQ, Qwen and Qwen "
#~ "2.5VL. See the [official doc](https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev/tutorials/index.html) for"
#~ " detail"
#~ msgstr ""
#~ "[æ–‡æ¡£] å¢åŠ äº†æ›´å¤šçš„æ¨¡å‹æ•™ç¨‹ï¼ŒåŒ…æ‹¬ DeepSeekã€QwQã€Qwen å’Œ Qwen"
#~ " 2.5VLã€‚è¯¦æƒ…è¯·å‚è§[å®˜æ–¹æ–‡æ¡£](https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.3-dev/tutorials/index.html)ã€‚"

#~ msgid ""
#~ "Pin modelscope<1.23.0 on vLLM v0.7.3 to"
#~ " resolve: https://github.com/vllm-"
#~ "project/vllm/pull/13807"
#~ msgstr ""
#~ "åœ¨ vLLM v0.7.3 ä¸Šé”å®š modelscope ç‰ˆæœ¬ä½äº "
#~ "1.23.0ï¼Œä»¥è§£å†³ï¼šhttps://github.com/vllm-project/vllm/pull/13807"

#~ msgid "Known issues"
#~ msgstr "å·²çŸ¥é—®é¢˜"

#~ msgid ""
#~ "In [some cases](https://github.com/vllm-project"
#~ "/vllm-kunlun/issues/324), especially when the "
#~ "input/output is very long, the accuracy"
#~ " of output may be incorrect. We "
#~ "are working on it. It'll be fixed"
#~ " in the next release."
#~ msgstr ""
#~ "åœ¨[æŸäº›æƒ…å†µä¸‹](https://github.com/vllm-project/vllm-"
#~ "kunlun/issues/324)ï¼Œç‰¹åˆ«æ˜¯å½“è¾“å…¥æˆ–è¾“å‡ºéå¸¸é•¿æ—¶ï¼Œè¾“å‡ºçš„å‡†ç¡®æ€§å¯èƒ½ä¼šæœ‰è¯¯ã€‚æˆ‘ä»¬æ­£åœ¨è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å°†åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ä¸­ä¿®å¤ã€‚"

#~ msgid ""
#~ "Improved and reduced the garbled code"
#~ " in model output. But if you "
#~ "still hit the issue, try to change"
#~ " the generation config value, such as"
#~ " `temperature`, and try again. There "
#~ "is also a knonwn issue shown "
#~ "below. Any [feedback](https://github.com/vllm-"
#~ "project/vllm-kunlun/issues/267) is welcome. "
#~ "[#277](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/277)"
#~ msgstr ""
#~ "æ”¹è¿›å¹¶å‡å°‘äº†æ¨¡å‹è¾“å‡ºä¸­çš„ä¹±ç é—®é¢˜ã€‚ä½†å¦‚æœä½ ä»ç„¶é‡åˆ°è¯¥é—®é¢˜ï¼Œè¯·å°è¯•æ›´æ”¹ç”Ÿæˆé…ç½®çš„å‚æ•°ï¼Œä¾‹å¦‚ "
#~ "`temperature`ï¼Œç„¶åå†è¯•ä¸€æ¬¡ã€‚ä¸‹é¢è¿˜åˆ—å‡ºäº†ä¸€ä¸ªå·²çŸ¥é—®é¢˜ã€‚æ¬¢è¿æä¾›ä»»ä½•[åé¦ˆ](https://github.com"
#~ "/vllm-project/vllm-"
#~ "kunlun/issues/267)ã€‚[#277](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/277)"

#~ msgid "v0.7.1rc1 - 2025.02.19"
#~ msgstr "v0.7.1rc1 - 2025.02.19"

#~ msgid ""
#~ "We are excited to announce the "
#~ "first release candidate of v0.7.1 for"
#~ " vllm-kunlun."
#~ msgstr "æˆ‘ä»¬å¾ˆé«˜å…´åœ°å®£å¸ƒ vllm-kunlun v0.7.1 çš„ç¬¬ä¸€ä¸ªå€™é€‰ç‰ˆæœ¬å‘å¸ƒã€‚"

#~ msgid ""
#~ "vLLM Kunlun Plugin (vllm-kunlun) is "
#~ "a community maintained hardware plugin "
#~ "for running vLLM on the Kunlun "
#~ "XPU. With this release, users can "
#~ "now enjoy the latest features and "
#~ "improvements of vLLM on the Kunlun "
#~ "XPU."
#~ msgstr ""
#~ "vLLM Kunlun æ’ä»¶ï¼ˆvllm-kunlunï¼‰æ˜¯ä¸€ä¸ªç”±ç¤¾åŒºç»´æŠ¤çš„ç¡¬ä»¶æ’ä»¶ï¼Œç”¨äºåœ¨ "
#~ "Kunlun XPU ä¸Šè¿è¡Œ vLLMã€‚é€šè¿‡æ­¤ç‰ˆæœ¬ï¼Œç”¨æˆ·ç°åœ¨å¯ä»¥åœ¨ Kunlun "
#~ "XPU ä¸Šäº«å—åˆ° vLLM çš„æœ€æ–°åŠŸèƒ½å’Œæ”¹è¿›ã€‚"

#~ msgid ""
#~ "Please follow the [official doc](https"
#~ "://vllm-kunlun.readthedocs.io/en/v0.7.1-dev) to start"
#~ " the journey. Note that this is "
#~ "a release candidate, and there may "
#~ "be some bugs or issues. We "
#~ "appreciate your feedback and suggestions "
#~ "[here](https://github.com/vllm-project/vllm-"
#~ "kunlun/issues/19)"
#~ msgstr ""
#~ "è¯·å‚é˜…[å®˜æ–¹æ–‡æ¡£](https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.1-dev)å¼€å§‹æ‚¨çš„ä½“éªŒä¹‹æ—…ã€‚è¯·æ³¨æ„ï¼Œè¿™æ˜¯ä¸€ä¸ªå€™é€‰å‘å¸ƒç‰ˆæœ¬ï¼Œå¯èƒ½ä¼šæœ‰ä¸€äº›æ¼æ´æˆ–é—®é¢˜ã€‚æˆ‘ä»¬éå¸¸æ¬¢è¿æ‚¨åœ¨[è¿™é‡Œ](https://github.com"
#~ "/vllm-project/vllm-kunlun/issues/19)æäº¤åé¦ˆå’Œå»ºè®®ã€‚"

#~ msgid ""
#~ "Initial supports for Kunlun XPU on "
#~ "vLLM. [#3](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/3)"
#~ msgstr ""
#~ "åœ¨ vLLM ä¸Šåˆæ­¥æ”¯æŒ Kunlun "
#~ "XPUã€‚[#3](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/3)"

#~ msgid ""
#~ "DeepSeek is now supported. "
#~ "[#88](https://github.com/vllm-project/vllm-kunlun/pull/88)"
#~ " [#68](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/68)"
#~ msgstr ""
#~ "ç°åœ¨å·²æ”¯æŒ DeepSeekã€‚ [#88](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/88) [#68](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/68)"

#~ msgid ""
#~ "Qwen, Llama series and other popular "
#~ "models are also supported, you can "
#~ "see more details in [here](https://vllm-"
#~ "kunlun.readthedocs.io/en/latest/user_guide/supported_models.html)."
#~ msgstr ""
#~ "Qwenã€Llama ç³»åˆ—åŠå…¶ä»–æµè¡Œçš„æ¨¡å‹ä¹Ÿå—æ”¯æŒï¼Œæ›´å¤šè¯¦æƒ…å¯å‚è§[è¿™é‡Œ](https://vllm-"
#~ "kunlun.readthedocs.io/en/latest/user_guide/supported_models.html)ã€‚"

#~ msgid ""
#~ "Added the Kunlun quantization config "
#~ "option, the implementation will coming "
#~ "soon. [#7](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/7) [#73](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/73)"
#~ msgstr ""
#~ "æ–°å¢äº† Kunlun é‡åŒ–é…ç½®é€‰é¡¹ï¼Œå…·ä½“å®ç°å³å°†æ¨å‡ºã€‚[#7](https://github.com"
#~ "/vllm-project/vllm-kunlun/pull/7) "
#~ "[#73](https://github.com/vllm-project/vllm-kunlun/pull/73)"

#~ msgid ""
#~ "Add silu_and_mul and rope ops and "
#~ "add mix ops into attention layer. "
#~ "[#18](https://github.com/vllm-project/vllm-kunlun/pull/18)"
#~ msgstr ""
#~ "æ·»åŠ  silu_and_mul å’Œ rope æ“ä½œï¼Œå¹¶å°†æ··åˆæ“ä½œåŠ å…¥åˆ° "
#~ "attention å±‚ã€‚ [#18](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/18)"

#~ msgid ""
#~ "[CI] Enable Kunlun CI to actively "
#~ "monitor and improve quality for vLLM "
#~ "on Kunlun. [#3](https://github.com/vllm-project"
#~ "/vllm-kunlun/pull/3)"
#~ msgstr ""
#~ "[CI] å¯ç”¨ Kunlun CIï¼Œä¸»åŠ¨ç›‘æµ‹å¹¶æå‡ vLLM åœ¨ "
#~ "Kunlun ä¸Šçš„è´¨é‡ã€‚[#3](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/3)"

#~ msgid ""
#~ "[Docker] Add vllm-kunlun container image"
#~ " [#64](https://github.com/vllm-project/vllm-"
#~ "kunlun/pull/64)"
#~ msgstr ""
#~ "[Docker] æ·»åŠ  vllm-kunlun å®¹å™¨é•œåƒ "
#~ "[#64](https://github.com/vllm-project/vllm-kunlun/pull/64)"

#~ msgid ""
#~ "[Docs] Add a [live doc](https://vllm-"
#~ "kunlun.readthedocs.org) [#55](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/55)"
#~ msgstr ""
#~ "[æ–‡æ¡£] æ·»åŠ äº†ä¸€ä¸ª [åœ¨çº¿æ–‡æ¡£](https://vllm-"
#~ "kunlun.readthedocs.org) [#55](https://github.com/vllm-"
#~ "project/vllm-kunlun/pull/55)"

#~ msgid ""
#~ "This release relies on an unreleased "
#~ "torch_npu version. It has been installed"
#~ " within official container image already."
#~ " Please [install](https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.1rc1/installation.html) it "
#~ "manually if you are using non-"
#~ "container environment."
#~ msgstr ""
#~ "æ­¤ç‰ˆæœ¬ä¾èµ–äºå°šæœªå‘å¸ƒçš„ torch_npu "
#~ "ç‰ˆæœ¬ã€‚è¯¥ç‰ˆæœ¬å·²é›†æˆåœ¨å®˜æ–¹å®¹å™¨é•œåƒä¸­ã€‚å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯éå®¹å™¨ç¯å¢ƒï¼Œè¯·[æ‰‹åŠ¨å®‰è£…](https://vllm-"
#~ "kunlun.readthedocs.io/en/v0.7.1rc1/installation.html)ã€‚"

#~ msgid ""
#~ "There are logs like `No platform "
#~ "detected, vLLM is running on "
#~ "UnspecifiedPlatform` or `Failed to import "
#~ "from vllm._C with ModuleNotFoundError(\"No "
#~ "module named 'vllm._C'\")` shown when "
#~ "running vllm-kunlun. It actually doesn't"
#~ " affect any functionality and performance."
#~ " You can just ignore it. And it"
#~ " has been fixed in this "
#~ "[PR](https://github.com/vllm-project/vllm/pull/12432) "
#~ "which will be included in v0.7.3 "
#~ "soon."
#~ msgstr ""
#~ "åœ¨è¿è¡Œ vllm-kunlun æ—¶ï¼Œä¼šæ˜¾ç¤ºç±»ä¼¼ `No platform "
#~ "detected, vLLM is running on "
#~ "UnspecifiedPlatform` æˆ– `Failed to import "
#~ "from vllm._C with ModuleNotFoundError(\"No "
#~ "module named 'vllm._C'\")` "
#~ "çš„æ—¥å¿—ã€‚è¿™å®é™…ä¸Šä¸ä¼šå½±å“ä»»ä½•åŠŸèƒ½å’Œæ€§èƒ½ï¼Œä½ å¯ä»¥ç›´æ¥å¿½ç•¥å®ƒã€‚è¿™ä¸ªé—®é¢˜å·²åœ¨æ­¤ [PR](https://github.com"
#~ "/vllm-project/vllm/pull/12432) ä¸­ä¿®å¤ï¼Œå¹¶å¾ˆå¿«ä¼šåœ¨ v0.7.3 "
#~ "ç‰ˆæœ¬ä¸­åŒ…å«ã€‚"

#~ msgid ""
#~ "There are logs like `# CPU blocks:"
#~ " 35064, # CPU blocks: 2730` shown "
#~ "when running vllm-kunlun which should"
#~ " be `# XPU blocks:` . It "
#~ "actually doesn't affect any functionality "
#~ "and performance. You can just ignore "
#~ "it. And it has been fixed in "
#~ "this [PR](https://github.com/vllm-project/vllm/pull/13378)"
#~ " which will be included in v0.7.3 "
#~ "soon."
#~ msgstr ""
#~ "åœ¨è¿è¡Œ vllm-kunlun æ—¶ï¼Œä¼šæ˜¾ç¤ºç±»ä¼¼ `# CPU "
#~ "blocks: 35064, # CPU blocks: 2730` "
#~ "çš„æ—¥å¿—ï¼Œå®é™…åº”è¯¥ä¸º `# XPU "
#~ "blocks:`ã€‚è¿™å®é™…ä¸Šä¸ä¼šå½±å“ä»»ä½•åŠŸèƒ½å’Œæ€§èƒ½ï¼Œä½ å¯ä»¥å¿½ç•¥å®ƒã€‚è¯¥é—®é¢˜å·²åœ¨è¿™ä¸ª [PR](https://github.com"
#~ "/vllm-project/vllm/pull/13378) ä¸­ä¿®å¤ï¼Œå¹¶å°†åœ¨ v0.7.3 "
#~ "ç‰ˆæœ¬ä¸­åŒ…å«ã€‚"

