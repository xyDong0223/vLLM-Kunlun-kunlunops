# Multi XPU (Qwen2.5-VL-32B)

## Run vllm-kunlun on Multi XPU

Setup environment using container:

```bash
# !/bin/bash
# rundocker.sh
XPU_NUM=8
DOCKER_DEVICE_CONFIG=""
if [ $XPU_NUM -gt 0 ]; then
    for idx in $(seq 0 $((XPU_NUM-1))); do
        DOCKER_DEVICE_CONFIG="${DOCKER_DEVICE_CONFIG} --device=/dev/xpu${idx}:/dev/xpu${idx}"
    done
    DOCKER_DEVICE_CONFIG="${DOCKER_DEVICE_CONFIG} --device=/dev/xpuctrl:/dev/xpuctrl"
fi

export build_image="xxxxxxxxxxxxxxxxx"

docker run -itd ${DOCKER_DEVICE_CONFIG} \
    --net=host \
    --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \
    --tmpfs /dev/shm:rw,nosuid,nodev,exec,size=32g \
    --cap-add=SYS_PTRACE \
    -v /home/users/vllm-kunlun:/home/vllm-kunlun \
    -v /usr/local/bin/xpu-smi:/usr/local/bin/xpu-smi \
    --name "$1" \
    -w /workspace \
    "$build_image" /bin/bash
```

### Offline Inference on Multi XPU

Start the server in a container:

```bash
from vllm import LLM, SamplingParams

def main():

    model_path = "/models/Qwen2.5-VL-32B-Instruct"

    llm_params = {
        "model": model_path,
        "tensor_parallel_size": 2,
        "trust_remote_code": True,
        "dtype": "float16",
        "enable_chunked_prefill": False,
        "enable_prefix_caching": False,
        "distributed_executor_backend": "mp",
        "max_model_len": 16384,
        "gpu_memory_utilization": 0.9,
    }

    llm = LLM(**llm_params)

    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "ä½ å¥½ï¼ä½ æ˜¯è°ï¼Ÿ"
                }
            ]
        }
    ]

    sampling_params = SamplingParams(
        max_tokens=200,
        temperature=0.7,
        top_k=50,
        top_p=0.9
    )

    outputs = llm.chat(messages, sampling_params=sampling_params)

    response = outputs[0].outputs[0].text
    print("=" * 50)
    print("Input content:", messages)
    print("Model response:\n", response)
    print("=" * 50)

if __name__ == "__main__":
    main()

```
:::::
If you run this script successfully, you can see the info shown below:
```bash
==================================================
Input content: [{'role': 'user', 'content': [{'type': 'text', 'text': 'ä½ å¥½ï¼ä½ æ˜¯è°ï¼Ÿ'}]}]
Model response:
 ä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚ä½ å¯ä»¥å«æˆ‘Qwenã€‚æˆ‘èƒ½å¤Ÿå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œæ¯”å¦‚å†™æ•…äº‹ã€å†™å…¬æ–‡ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ã€é€»è¾‘æ¨ç†ã€ç¼–ç¨‹ç­‰ç­‰ï¼Œè¿˜èƒ½è¡¨è¾¾è§‚ç‚¹ï¼Œç©æ¸¸æˆç­‰ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ ğŸ˜Š
==================================================
```
### Online Serving on Multi XPU
Start the vLLM server on a multi XPU:
```bash
python -m vllm.entrypoints.openai.api_server \
    --host 0.0.0.0 \
    --port 9988 \
    --model /models/Qwen2.5-VL-32B-Instruct \
    --gpu-memory-utilization 0.9 \
    --trust-remote-code \
    --max-model-len 32768 \
    --tensor-parallel-size 2 \
    --dtype float16 \
    --max_num_seqs 128 \
    --max_num_batched_tokens 32768 \
    --block-size 128 \
    --no-enable-prefix-caching \
    --no-enable-chunked-prefill \
    --distributed-executor-backend mp \
    --served-model-name Qwen2.5-VL-32B-Instruct \
    --compilation-config '{"splitting_ops": ["vllm.unified_attention", 
                                                "vllm.unified_attention_with_output",
                                                "vllm.unified_attention_with_output_kunlun",
                                                "vllm.mamba_mixer2",
                                                "vllm.mamba_mixer", 
                                                "vllm.short_conv", 
                                                "vllm.linear_attention", 
                                                "vllm.plamo2_mamba_mixer", 
                                                "vllm.gdn_attention", 
                                                "vllm.sparse_attn_indexer"]}'
                                                #Version 0.11.0 
```
If your service start successfully, you can see the info shown below:
```bash
(APIServer pid=110552) INFO:     Started server process [110552]
(APIServer pid=110552) INFO:     Waiting for application startup.
(APIServer pid=110552) INFO:     Application startup complete.
```
Once your server is started, you can query the model with input prompts:
```bash
curl http://localhost:9988/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen2.5-VL-32B-Instruct",
        "prompt": "ä½ å¥½ï¼ä½ æ˜¯è°?",
        "max_tokens": 100,
        "temperature": 0.7
    }'
```
If you query the server successfully, you can see the info shown below (client):
```bash
{"id":"cmpl-9784668ac5bc4b4e975d0aa5ee8377c6","object":"text_completion","created":1768898088,"model":"Qwen2.5-VL-32B-Instruct","choices":[{"index":0,"text":" ä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚ä½ å¯ä»¥å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œå¦‚å†™æ•…äº‹ã€å…¬æ–‡ã€é‚®ä»¶ã€å‰§æœ¬ç­‰ï¼Œè¿˜èƒ½è¡¨è¾¾\n","logprobs":null,"finish_reason":"stop","stop_reason":null,"token_ids":null,"prompt_logprobs":null,"prompt_token_ids":null}],"service_tier":null,"system_fingerprint":null,"usage":{"prompt_tokens":5,"total_tokens":45,"completion_tokens":40,"prompt_tokens_details":null},"kv_transfer_params":null}
```
Logs of the vllm server:
```bash
(APIServer pid=110552) INFO 01-20 16:34:48 [loggers.py:127] Engine 000: Avg prompt throughput: 0.5 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=110552) INFO:     127.0.0.1:17988 - "POST /v1/completions HTTP/1.1" 200 OK
(APIServer pid=110552) INFO 01-20 16:34:58 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 3.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=110552) INFO 01-20 16:35:08 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
```
Input an image for testing.Here,a python script is used:
```python
import requests
import base64

API_URL = "http://localhost:9988/v1/chat/completions"
MODEL_NAME = "Qwen2.5-VL-32B-Instruct"
IMAGE_PATH = "/images.jpeg"

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

base64_image = encode_image(IMAGE_PATH)

payload = {
    "model": MODEL_NAME,
    "messages": [
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "ä½ å¥½ï¼è¯·æè¿°ä¸€ä¸‹è¿™å¼ å›¾ç‰‡ã€‚"
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{base64_image}"
                    }
                }
            ]
        }
    ],
    "max_tokens": 300,
    "temperature": 0.1,
    "top_p": 0.9,
    "top_k": 50
}

response = requests.post(API_URL, json=payload)
print(response.json())
```
If you query the server successfully, you can see the info shown below (client):
```bash
{'id': 'chatcmpl-9857119aed664a3e8f078efd90defdca', 'object': 'chat.completion', 'created': 1768898198, 'model': 'Qwen2.5-VL-32B-Instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'ä½ å¥½ï¼è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä¸ªæ ‡å¿—ï¼Œå†…å®¹å¦‚ä¸‹ï¼š\n\n1. **å·¦ä¾§å›¾æ ‡**ï¼š\n   - ä¸€ä¸ªé»„è‰²çš„åœ†å½¢ç¬‘è„¸è¡¨æƒ…ç¬¦å·ã€‚\n   - ç¬‘è„¸çš„è¡¨æƒ…éå¸¸å¼€å¿ƒï¼Œçœ¼ç›çœ¯æˆå¼¯å¼¯çš„å½¢çŠ¶ï¼Œå˜´å·´å¼ å¼€éœ²å‡ºç‰™é½¿ï¼Œæ˜¾å¾—éå¸¸æ„‰å¿«ã€‚\n   - ç¬‘è„¸çš„åŒæ‰‹åœ¨èƒ¸å‰åšå‡ºæ‹¥æŠ±çš„åŠ¨ä½œï¼Œæ‰‹æŒæœå¤–ï¼Œè±¡å¾ç€â€œæ‹¥æŠ±â€æˆ–â€œå‹å¥½çš„å§¿æ€â€ã€‚\n\n2. **å³ä¾§æ–‡å­—**ï¼š\n   - æ–‡å­—æ˜¯è‹±æ–‡å•è¯ï¼šâ€œHugging Faceâ€ã€‚\n   - å­—ä½“ä¸ºé»‘è‰²ï¼Œå­—ä½“é£æ ¼ç®€æ´ã€ç°ä»£ï¼Œçœ‹èµ·æ¥åƒæ˜¯æ— è¡¬çº¿å­—ä½“ï¼ˆsans-serifï¼‰ã€‚\n\n3. **æ•´ä½“è®¾è®¡**ï¼š\n   - æ•´ä¸ªæ ‡å¿—çš„è®¾è®¡éå¸¸ç®€æ´æ˜äº†ï¼Œé¢œè‰²å¯¹æ¯”é²œæ˜ï¼ˆé»„è‰²ç¬‘è„¸å’Œé»‘è‰²æ–‡å­—ï¼‰ï¼ŒèƒŒæ™¯ä¸ºçº¯ç™½è‰²ï¼Œç»™äººä¸€ç§å¹²å‡€ã€å‹å¥½çš„æ„Ÿè§‰ã€‚\n   - ç¬‘è„¸å’Œæ–‡å­—ä¹‹é—´çš„é—´è·é€‚ä¸­ï¼Œå¸ƒå±€å¹³è¡¡ã€‚\n\nè¿™ä¸ªæ ‡å¿—å¯èƒ½å±äºæŸä¸ªå“ç‰Œæˆ–ç»„ç»‡ï¼Œåå­—ä¸ºâ€œHugging Faceâ€ï¼Œä»è®¾è®¡æ¥çœ‹ï¼Œå®ƒä¼ è¾¾äº†ä¸€ç§å‹å¥½ã€å¼€æ”¾å’Œç§¯æçš„å½¢è±¡ã€‚', 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning_content': None}, 'logprobs': None, 'finish_reason': 'stop', 'stop_reason': None, 'token_ids': None}], 'service_tier': None, 'system_fingerprint': None, 'usage': {'prompt_tokens': 95, 'total_tokens': 311, 'completion_tokens': 216, 'prompt_tokens_details': None}, 'prompt_logprobs': None, 'prompt_token_ids': None, 'kv_transfer_params': None}
```
Logs of the vllm server:
```bash
(APIServer pid=110552) INFO:     127.0.0.1:19378 - "POST /v1/chat/completions HTTP/1.1" 200 OK
(APIServer pid=110552) INFO 01-20 16:36:49 [loggers.py:127] Engine 000: Avg prompt throughput: 9.5 tokens/s, Avg generation throughput: 21.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
(APIServer pid=110552) INFO 01-20 16:36:59 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
```